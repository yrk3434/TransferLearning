{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7. 텍스트 문서의 범주화 - (3) 리뷰 감성 분류기 구현\n",
    "\n",
    "\n",
    "- 이제 앞에서 구현한 CNN 문서 모델을 훈련해서 감성 분류기를 구축해 보자\n",
    "- 캐글에서 아마존 감성 분석 리뷰 데이터 세트를 다운로드 받아 압축해제하여 저장한다. (train.ft.txt와 test.ft.txt 두 파일 모두 다운)\n",
    "    - 다운로드 url\n",
    "        - https://www.kaggle.com/bittlingmayer/amazonreviews\n",
    "    - 저장경로\n",
    "        - train.ft.txt -> data/amazonreviews/train.ft/train.ft.txt\n",
    "        - test.ft.txt -> data/amazonreviews/test.ft/test.ft.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazonreviews.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "# !kaggle datasets download -d bittlingmayer/amazonreviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  amazonreviews.zip\n",
      "  inflating: ./data/amazonreviews/test.ft.txt.bz2  \n",
      "  inflating: ./data/amazonreviews/train.ft.txt.bz2  \n"
     ]
    }
   ],
   "source": [
    "# !unzip amazonreviews.zip -d ./data/amazonreviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !bzip2 ./data/amazonreviews/test.ft.txt.bz2 -d \n",
    "# !bzip2 ./data/amazonreviews/train.ft.txt.bz2 -d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import config\n",
    "from dataloader.loader import Loader\n",
    "from preprocessing.utils import Preprocess, remove_empty_docs\n",
    "from dataloader.embeddings import GloVe\n",
    "from model.cnn_document_model import DocumentModel, TrainingParameters\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 아마존 리뷰 데이터 로드\n",
    "\n",
    "- 아마존 리뷰 데이터를 로드한다. (data/amazonreviews 경로)\n",
    "    - 360만개의 훈련 샘플과 40만개의 테스트 샘플이 있다. train 데이터셋은 랜덤으로 20만개만 추출하여 사용한다\n",
    "    - <b>\\__label__1</b>은 별점 1-2점을 매긴 리뷰에 해당, <b>\\__label__2</b>는 별점 4-5점을 매긴 리뷰에 해당한다\n",
    "    - 별점 3점의 리뷰, 즉, 중립적인 감성을 가진 리뷰는 이 데이터 세트에 포함되지 않았다\n",
    "    - 원본 데이터 예시\n",
    "````\n",
    "__label__<X> <summary/title>: <Review Text>\n",
    "Example:\n",
    "__label__2 Good Movie: Awesome.... simply awesome. I couldn't put this down\n",
    "and laughed, smiled, and even got tears! A brand new favorite author.\n",
    "```\n",
    "\n",
    "- 아마존 리뷰 데이터를 데이터프레임으로 변환한다\n",
    "    - sentiment 칼럼에 0(부정) 또는 1(긍정) 값을 입력\n",
    "    - 데이터프레임 예시\n",
    "```\n",
    "index   review                                              sentiment\n",
    "0       Stuning even for the non-gamer . This sound t...    1\n",
    "1       The best soundtrack ever to anything. . I'm r...    1\n",
    "2       Amazing! . This soundtrack is my favorite mus...    1\n",
    "3       Excellent Soundtrack: I truly like this soundt...   1\n",
    "4       Remember, Pull Your Jaw Off The Floor After He...   1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape : (3600000, 2)\n",
      "test_df.shape : (400000, 2)\n"
     ]
    }
   ],
   "source": [
    "# dataloader/loader.py 의 Loader.load_amazon_reviews 참고\n",
    "\n",
    "# 아마존 리뷰 데이터를 로드하여 데이터프레임으로 변환한다\n",
    "train_df = Loader.load_amazon_reviews('train')\n",
    "print(f'train_df.shape : {train_df.shape}')\n",
    "\n",
    "test_df = Loader.load_amazon_reviews('test')\n",
    "print(f'test_df.shape : {test_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "1    100020\n",
       "0     99980\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습셋에서 랜덤으로 20만개만 추출하여 feature 추출에 사용한다\n",
    "dataset = train_df.sample(n=200000, random_state=42)\n",
    "dataset.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stuning even for the non-gamer .  This sound t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The best soundtrack ever to anything. .  I'm r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazing! .  This soundtrack is my favorite mus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  Stuning even for the non-gamer .  This sound t...          1\n",
       "1  The best soundtrack ever to anything. .  I'm r...          1\n",
       "2  Amazing! .  This soundtrack is my favorite mus...          1\n",
       "3  Excellent Soundtrack: I truly like this soundt...          1\n",
       "4  Remember, Pull Your Jaw Off The Floor After He...          1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인덱스 시퀀스 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus.shape : (200000,)\n",
      "target.shape : (200000,)\n",
      "=== after remove_empty_docs ===\n",
      "corpus size : 200000\n",
      "target size : 200000\n"
     ]
    }
   ],
   "source": [
    "# 추출한 20만개 데이터 샘플에서 review, sentiment 칼럼 값들 추출\n",
    "corpus = dataset['review'].values\n",
    "target = dataset['sentiment'].values\n",
    "print(f'corpus.shape : {corpus.shape}')\n",
    "print(f'target.shape : {target.shape}')\n",
    "\n",
    "# 유효하지 않은 값 제거 (비어있거나 길이가 30 이하인 경우 제거)\n",
    "corpus, target = remove_empty_docs(corpus, target)\n",
    "print('=== after remove_empty_docs ===')\n",
    "print(f'corpus size : {len(corpus)}')\n",
    "print(f'target size : {len(target)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 43195 unique tokens.\n",
      "All documents processed.ocessed."
     ]
    }
   ],
   "source": [
    "# 20만개 데이터 샘플에 대해 인덱스 사전 구축 및 인덱스 시퀀스 변환\n",
    "preprocessor = Preprocess(corpus=corpus)\n",
    "corpus_to_seq = preprocessor.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus_to_seq size : 200000\n",
      "corpus_to_seq[0] size : 300\n",
      "corpus_to_seq[0] :\n",
      "[ 2  3  4  5  6  7  8  9  7 10 11 12 13 14 15 16 17 18 19 20 21  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0 22 23 24 25 26 27 28 29  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 16 30 31 32 33 34\n",
      " 17 30 35 36 37 14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      " 38 39 40 41 42 37 16 43 44 45 46 17 37 47 48 37 49  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0 18 19 20 30 50 51 52 17 53 54 46 55 56 36 57  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0 37 58 59  8 60 39 61 62 63 64 65 59\n",
      " 66 41 67 68 28 69 17 70 71 72  0  0  0  0  0  0  0  0 39 61 73 74 75 76\n",
      "  4  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(f'corpus_to_seq size : {len(corpus_to_seq)}')\n",
    "print(f'corpus_to_seq[0] size : {len(corpus_to_seq[0])}')\n",
    "print(f'corpus_to_seq[0] :')\n",
    "print(corpus_to_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Expensive Junk: This product consists of a piece of thin flexible insulating material, adhesive backed velcro and white electrical tape.Problems . 1. Instructions are three pictures with little more information.2. Velcro was all crumpled as received and was stronger than the adhesive. When i tried to disengage the velcro both pieces came off and the paint from the ceiling.3. White electrical tape was horrible... cheap, narrow and it fell off in less than 1 hour.4. The price is a ripoff.I am building my own which is easier to use, cheaper, more attractive, and higher r-value. I am surprised Amazon even lists this junk.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인덱싱되기 전 원본 문서\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holdout_corpus.shape : (400000,)\n",
      "holdout_target.shape : (400000,)\n",
      "=== after remove_empty_docs ===\n",
      "holdout_corpus size : 400000\n",
      "holdout_target size : 400000\n"
     ]
    }
   ],
   "source": [
    "# 테스트셋(test_df) 40만건 리뷰에서 review, sentiment 칼럼 값 추출\n",
    "holdout_corpus = test_df['review'].values\n",
    "holdout_target = test_df['sentiment'].values\n",
    "print(f'holdout_corpus.shape : {holdout_corpus.shape}')\n",
    "print(f'holdout_target.shape : {holdout_target.shape}')\n",
    "\n",
    "# 유효하지 않은 값 제거 (비어있거나 길이가 30 이하인 경우 제거)\n",
    "holdout_corpus, holdout_target = remove_empty_docs(holdout_corpus, holdout_target)\n",
    "print('=== after remove_empty_docs ===')\n",
    "print(f'holdout_corpus size : {len(holdout_corpus)}')\n",
    "print(f'holdout_target size : {len(holdout_target)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All documents processed.ocessed."
     ]
    }
   ],
   "source": [
    "# 테스트셋을 인덱스 시퀀스로 변환 (위에서 생성한 인덱스 사전 그대로 사용)\n",
    "holdout_corpus_to_seq = preprocessor.transform(holdout_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holdout_corpus_to_seq size : 400000\n",
      "holdout_corpus_to_seq[0] size : 300\n",
      "holdout_corpus_to_seq[0] :\n",
      "[  335   336     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0    63  4565  6750   132   120     7\n",
      "    37   335  2779     7   244  3736     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    39    91   569    41     4   336    83   765    17    39   670  1043\n",
      "    53     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0    38    39   449    55     8   238\n",
      "  9021    53   262   214  1112   131     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     8   178  9021   184 19876   118  5560    55    37  1317     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     4   336   184  8860   172     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "   583    23    17   584   184  4719     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0   120     7   172   241  1812  2542\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     4    59     8  6845  7235   336    55    63   167     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0   957   507   820   416    53  1120\n",
      "    59   184  2967   214     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "print(f'holdout_corpus_to_seq size : {len(holdout_corpus_to_seq)}')\n",
    "print(f'holdout_corpus_to_seq[0] size : {len(holdout_corpus_to_seq[0])}')\n",
    "print(f'holdout_corpus_to_seq[0] :')\n",
    "print(holdout_corpus_to_seq[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 임베딩 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 50 dim GloVe vectors\n",
      "Found 400000 word vectors.\n",
      "words not found in embeddings: 2582\n"
     ]
    }
   ],
   "source": [
    "# 인덱싱된 텍스트 데이터를 GloVe로 임베딩 초기화.\n",
    "# glove.6B.50d.txt에 없는 단어는 OOV..txt에 write한다\n",
    "# word_index는 {'expensive': 2, 'junk': 3, 'this': 4, ...} 형태의 인덱싱 사전\n",
    "glove = GloVe(50)\n",
    "initial_embeddings = glove.get_embedding(preprocessor.word_index)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43195"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인덱스 사전의 단어 수\n",
    "len(preprocessor.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43197, 50)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GloVe로 임베딩 초기화된 행렬. 벡터 개수는 word_index 인덱스 사전의 단어 + 2, 차원 수는 50이다\n",
    "initial_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "glove6B.50d의 단어 수는 40만개이며, 이 중 아마존 리뷰 데이터 속 4만 3천여개 단어에 대한 임베딩 행렬을 생성하였다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN 감성분석 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size = 43197  and the index of vocabulary words passed has 43195 words\n"
     ]
    }
   ],
   "source": [
    "# model/cnn_document_model.py의 DocumentModel 클래스 참조\n",
    "\n",
    "# CNN 기반 문서 분류 모델 인스턴스 생성. 위에서 GloVe로 만든 임베딩 행렬을 임베딩 초깃값으로 사용한다\n",
    "amazon_review_model = DocumentModel(vocab_size=preprocessor.get_vocab_size(),\n",
    "                                    word_index=preprocessor.word_index,\n",
    "                                    num_sentences=Preprocess.NUM_SENTENCES,\n",
    "                                    embedding_weights=initial_embeddings,\n",
    "                                    conv_activation='tanh',\n",
    "                                    hidden_dims=64,\n",
    "                                    input_dropout=0.40,\n",
    "                                    hidden_gaussian_noise_sd=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape : (200000, 300)\n",
      "y_train.shape : (200000,)\n",
      "x_test.shape : (400000, 300)\n",
      "y_test.shape : (400000,)\n",
      "Epoch 1/35\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.68892, saving model to ./checkpoint/amazonreviews/model_06.hdf5\n",
      "2969/2969 - 77s - loss: 0.6941 - accuracy: 0.5088 - val_loss: 0.6889 - val_accuracy: 0.5062 - 77s/epoch - 26ms/step\n",
      "Epoch 2/35\n",
      "\n",
      "Epoch 2: val_loss improved from 0.68892 to 0.68468, saving model to ./checkpoint/amazonreviews/model_06.hdf5\n",
      "2969/2969 - 78s - loss: 0.6890 - accuracy: 0.5159 - val_loss: 0.6847 - val_accuracy: 0.5254 - 78s/epoch - 26ms/step\n",
      "Epoch 3/35\n",
      "\n",
      "Epoch 3: val_loss improved from 0.68468 to 0.68351, saving model to ./checkpoint/amazonreviews/model_06.hdf5\n",
      "2969/2969 - 79s - loss: 0.6868 - accuracy: 0.5197 - val_loss: 0.6835 - val_accuracy: 0.5250 - 79s/epoch - 27ms/step\n",
      "Epoch 4/35\n",
      "\n",
      "Epoch 4: val_loss improved from 0.68351 to 0.68340, saving model to ./checkpoint/amazonreviews/model_06.hdf5\n",
      "2969/2969 - 78s - loss: 0.6859 - accuracy: 0.5218 - val_loss: 0.6834 - val_accuracy: 0.5261 - 78s/epoch - 26ms/step\n",
      "Epoch 5/35\n",
      "\n",
      "Epoch 5: val_loss improved from 0.68340 to 0.68218, saving model to ./checkpoint/amazonreviews/model_06.hdf5\n",
      "2969/2969 - 81s - loss: 0.6850 - accuracy: 0.5229 - val_loss: 0.6822 - val_accuracy: 0.5263 - 81s/epoch - 27ms/step\n",
      "Epoch 6/35\n",
      "\n",
      "Epoch 6: val_loss improved from 0.68218 to 0.68214, saving model to ./checkpoint/amazonreviews/model_06.hdf5\n",
      "2969/2969 - 79s - loss: 0.6844 - accuracy: 0.5239 - val_loss: 0.6821 - val_accuracy: 0.5261 - 79s/epoch - 27ms/step\n",
      "Epoch 7/35\n",
      "\n",
      "Epoch 7: val_loss improved from 0.68214 to 0.68163, saving model to ./checkpoint/amazonreviews/model_06.hdf5\n",
      "2969/2969 - 71s - loss: 0.6833 - accuracy: 0.5232 - val_loss: 0.6816 - val_accuracy: 0.5108 - 71s/epoch - 24ms/step\n",
      "Epoch 8/35\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.68163\n",
      "2969/2969 - 69s - loss: 0.6830 - accuracy: 0.5239 - val_loss: 0.6847 - val_accuracy: 0.5281 - 69s/epoch - 23ms/step\n",
      "Epoch 9/35\n",
      "\n",
      "Epoch 9: val_loss improved from 0.68163 to 0.68124, saving model to ./checkpoint/amazonreviews/model_06.hdf5\n",
      "2969/2969 - 74s - loss: 0.6825 - accuracy: 0.5258 - val_loss: 0.6812 - val_accuracy: 0.5281 - 74s/epoch - 25ms/step\n",
      "Epoch 10/35\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.68124\n",
      "2969/2969 - 69s - loss: 0.6824 - accuracy: 0.5249 - val_loss: 0.6813 - val_accuracy: 0.5282 - 69s/epoch - 23ms/step\n",
      "Epoch 11/35\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.68124\n",
      "2969/2969 - 67s - loss: 0.6818 - accuracy: 0.5263 - val_loss: 0.6814 - val_accuracy: 0.5274 - 67s/epoch - 23ms/step\n",
      "Epoch 12/35\n",
      "\n",
      "Epoch 12: val_loss improved from 0.68124 to 0.68002, saving model to ./checkpoint/amazonreviews/model_06.hdf5\n",
      "2969/2969 - 67s - loss: 0.6810 - accuracy: 0.5260 - val_loss: 0.6800 - val_accuracy: 0.5284 - 67s/epoch - 23ms/step\n",
      "Epoch 13/35\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.68002\n",
      "2969/2969 - 68s - loss: 0.6810 - accuracy: 0.5272 - val_loss: 0.6819 - val_accuracy: 0.5296 - 68s/epoch - 23ms/step\n",
      "Epoch 14/35\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.68002\n",
      "2969/2969 - 2350s - loss: 0.6805 - accuracy: 0.5279 - val_loss: 0.6810 - val_accuracy: 0.5295 - 2350s/epoch - 792ms/step\n",
      "Epoch 15/35\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.68002\n",
      "2969/2969 - 8678s - loss: 0.6804 - accuracy: 0.5271 - val_loss: 0.6802 - val_accuracy: 0.5281 - 8678s/epoch - 3s/step\n",
      "Epoch 16/35\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.68002\n",
      "2969/2969 - 12061s - loss: 0.6800 - accuracy: 0.5273 - val_loss: 0.6810 - val_accuracy: 0.5277 - 12061s/epoch - 4s/step\n",
      "Epoch 17/35\n",
      "\n",
      "Epoch 17: val_loss improved from 0.68002 to 0.67949, saving model to ./checkpoint/amazonreviews/model_06.hdf5\n",
      "2969/2969 - 11153s - loss: 0.6798 - accuracy: 0.5285 - val_loss: 0.6795 - val_accuracy: 0.5297 - 11153s/epoch - 4s/step\n",
      "Epoch 18/35\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.67949\n",
      "2969/2969 - 6305s - loss: 0.6793 - accuracy: 0.5286 - val_loss: 0.6796 - val_accuracy: 0.5297 - 6305s/epoch - 2s/step\n",
      "Epoch 19/35\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.67949\n",
      "2969/2969 - 11052s - loss: 0.6791 - accuracy: 0.5292 - val_loss: 0.6804 - val_accuracy: 0.5287 - 11052s/epoch - 4s/step\n",
      "Epoch 20/35\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.67949\n",
      "2969/2969 - 11538s - loss: 0.6793 - accuracy: 0.5279 - val_loss: 0.6814 - val_accuracy: 0.5291 - 11538s/epoch - 4s/step\n",
      "Epoch 21/35\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.67949\n",
      "2969/2969 - 12797s - loss: 0.6785 - accuracy: 0.5281 - val_loss: 0.6799 - val_accuracy: 0.5291 - 12797s/epoch - 4s/step\n",
      "Epoch 22/35\n",
      "\n",
      "Epoch 22: val_loss improved from 0.67949 to 0.67919, saving model to ./checkpoint/amazonreviews/model_06.hdf5\n",
      "2969/2969 - 10156s - loss: 0.6782 - accuracy: 0.5299 - val_loss: 0.6792 - val_accuracy: 0.5297 - 10156s/epoch - 3s/step\n",
      "Epoch 23/35\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.67919\n",
      "2969/2969 - 11717s - loss: 0.6783 - accuracy: 0.5288 - val_loss: 0.6796 - val_accuracy: 0.5302 - 11717s/epoch - 4s/step\n",
      "Epoch 24/35\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.67919\n",
      "2969/2969 - 12732s - loss: 0.6779 - accuracy: 0.5288 - val_loss: 0.6801 - val_accuracy: 0.5300 - 12732s/epoch - 4s/step\n",
      "Epoch 25/35\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.67919\n",
      "2969/2969 - 12811s - loss: 0.6772 - accuracy: 0.5303 - val_loss: 0.6822 - val_accuracy: 0.5296 - 12811s/epoch - 4s/step\n",
      "Epoch 26/35\n",
      "\n",
      "Epoch 26: val_loss improved from 0.67919 to 0.67888, saving model to ./checkpoint/amazonreviews/model_06.hdf5\n",
      "2969/2969 - 13940s - loss: 0.6767 - accuracy: 0.5299 - val_loss: 0.6789 - val_accuracy: 0.5304 - 13940s/epoch - 5s/step\n",
      "Epoch 27/35\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.67888\n",
      "2969/2969 - 15212s - loss: 0.6769 - accuracy: 0.5311 - val_loss: 0.6822 - val_accuracy: 0.5297 - 15212s/epoch - 5s/step\n",
      "Epoch 28/35\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.67888\n",
      "2969/2969 - 13707s - loss: 0.6766 - accuracy: 0.5309 - val_loss: 0.6799 - val_accuracy: 0.5305 - 13707s/epoch - 5s/step\n",
      "Epoch 29/35\n",
      "\n",
      "Epoch 29: val_loss improved from 0.67888 to 0.67855, saving model to ./checkpoint/amazonreviews/model_06.hdf5\n",
      "2969/2969 - 12926s - loss: 0.6765 - accuracy: 0.5306 - val_loss: 0.6785 - val_accuracy: 0.5298 - 12926s/epoch - 4s/step\n",
      "Epoch 30/35\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.67855\n",
      "2969/2969 - 10969s - loss: 0.6759 - accuracy: 0.5306 - val_loss: 0.6795 - val_accuracy: 0.5295 - 10969s/epoch - 4s/step\n",
      "Epoch 31/35\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.67855\n",
      "2969/2969 - 12574s - loss: 0.6756 - accuracy: 0.5305 - val_loss: 0.6811 - val_accuracy: 0.5124 - 12574s/epoch - 4s/step\n",
      "Epoch 32/35\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.67855\n",
      "2969/2969 - 11557s - loss: 0.6757 - accuracy: 0.5294 - val_loss: 0.6823 - val_accuracy: 0.5301 - 11557s/epoch - 4s/step\n",
      "Epoch 33/35\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.67855\n",
      "2969/2969 - 993s - loss: 0.6753 - accuracy: 0.5314 - val_loss: 0.6795 - val_accuracy: 0.5302 - 993s/epoch - 335ms/step\n",
      "Epoch 34/35\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.67855\n",
      "2969/2969 - 137s - loss: 0.6753 - accuracy: 0.5312 - val_loss: 0.6813 - val_accuracy: 0.5124 - 137s/epoch - 46ms/step\n",
      "Epoch 35/35\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.67855\n",
      "2969/2969 - 73s - loss: 0.6748 - accuracy: 0.5316 - val_loss: 0.6811 - val_accuracy: 0.5286 - 73s/epoch - 25ms/step\n"
     ]
    }
   ],
   "source": [
    "# 학습된 모델을 저장할 디렉토리 생성\n",
    "if not os.path.exists(os.path.join(config.MODEL_DIR, 'amazonreviews')):\n",
    "    os.makedirs(os.path.join(config.MODEL_DIR, 'amazonreviews'))\n",
    "\n",
    "# 학습 파라미터 저장 클래스\n",
    "train_params = TrainingParameters('model_with_tanh_activation', \n",
    "                                  model_file_path = config.MODEL_DIR + '/amazonreviews/model_06.hdf5',\n",
    "                                  model_hyper_parameters = config.MODEL_DIR + '/amazonreviews/model_06.json',\n",
    "                                  model_train_parameters = config.MODEL_DIR + '/amazonreviews/model_06_meta.json',\n",
    "                                  num_epochs=35)\n",
    "\n",
    "# 모델 컴파일\n",
    "amazon_review_model.get_classification_model().compile(loss=\"binary_crossentropy\", \n",
    "                                                       optimizer=train_params.optimizer,\n",
    "                                                       metrics=[\"accuracy\"])\n",
    "\n",
    "# callback (1) - 자동저장 체크포인트\n",
    "checkpointer = ModelCheckpoint(filepath=train_params.model_file_path,\n",
    "                               verbose=1,\n",
    "                               save_best_only=True,\n",
    "                               save_weights_only=True)\n",
    "\n",
    "# callback (2) - 조기종료\n",
    "early_stop = EarlyStopping(patience=2)\n",
    "\n",
    "# 모델에 입력할 학습데이터, 테스트데이터 (인덱스 값들의 시퀀스로 변환된 값)\n",
    "x_train = np.array(corpus_to_seq)\n",
    "y_train = np.array(target)\n",
    "x_test = np.array(holdout_corpus_to_seq)\n",
    "y_test = np.array(holdout_target)\n",
    "print(f'x_train.shape : {x_train.shape}')\n",
    "print(f'y_train.shape : {y_train.shape}')\n",
    "print(f'x_test.shape : {x_test.shape}')\n",
    "print(f'y_test.shape : {y_test.shape}')\n",
    "\n",
    "# 모델 훈련 시작\n",
    "amazon_review_model.get_classification_model().fit(x_train,\n",
    "                                                   y_train, \n",
    "                                                   batch_size=train_params.batch_size, \n",
    "                                                   epochs=train_params.num_epochs,  # 35 epochs\n",
    "                                                   verbose=2,\n",
    "                                                   validation_split=train_params.validation_split, # 5%\n",
    "                                                   callbacks=[checkpointer])\n",
    "\n",
    "# 모델 저장\n",
    "amazon_review_model._save_model(train_params.model_hyper_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 - 17s - loss: 0.6862 - accuracy: 0.5259 - 17s/epoch - 28ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6862403750419617, 0.5258899927139282]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 평가 - 테스트 데이터셋으로 수행\n",
    "amazon_review_model.get_classification_model().evaluate(x_test,\n",
    "                                                        y_test, \n",
    "                                                        train_params.batch_size*10,\n",
    "                                                        verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가장 많이 변경된 임베딩은 무엇일까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nothing', 5.898498454582604),\n",
       " ('never', 5.838235222411407),\n",
       " ('not', 5.837764425357621),\n",
       " ('money', 5.567684147550348),\n",
       " ('instead', 5.473179924519233),\n",
       " ('better', 5.338847887023589),\n",
       " ('bad', 5.176505421221354),\n",
       " ('waste', 4.991878514338513),\n",
       " ('guess', 4.919912188564288),\n",
       " ('back', 4.898684547565275),\n",
       " ('what', 4.791495606601605),\n",
       " ('return', 4.621494796555242),\n",
       " ('same', 4.567734369178881),\n",
       " ('worst', 4.561360698176675),\n",
       " ('something', 4.4871946257678745),\n",
       " ('poor', 4.479186207022457),\n",
       " ('sorry', 4.285149087759856),\n",
       " ('tried', 4.224736281244272),\n",
       " ('no', 4.189658446760836),\n",
       " ('boring', 4.187275067385223)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learned_embeddings = amazon_review_model.get_classification_model().get_layer('imdb_embedding').get_weights()[0]\n",
    "\n",
    "embd_change = {}\n",
    "for word, i in preprocessor.word_index.items():\n",
    "    # Frobenium norm (Euclidean norm) 계\n",
    "    embd_change[word] = np.linalg.norm(initial_embeddings[i]-learned_embeddings[i])\n",
    "embd_change = sorted(embd_change.items(), key=lambda x: x[1], reverse=True)\n",
    "embd_change[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "f541650a259d3c85c16fa389922248120f711c850d1b246e80025daa1aee6568"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
