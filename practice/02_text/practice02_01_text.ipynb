{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습 02 - 텍스트 문서 범주화에 전이학습 적용하기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이메일, 평점 등의 텍스트를 분류하는 데에 전이학습 이용\n",
    "- IMDB 영화 리뷰 데이터를 다운로드 받아 data 디렉토리에 압축 해제한다\n",
    "    - 다운로드 : http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "    - 저장경로 : data/practice02/aclImdb\n",
    "    - 폴더 구조\n",
    "        - data/aclImdb/train/pos/...txt\n",
    "        - data/aclImdb/train/neg/...txt\n",
    "- 사전학습 임베딩 파일 받기\n",
    "    - 사전 훈련된 Word2Vec 임베딩 불러오기(GloVe)\n",
    "    - 다운로드 링크 : http://nlp.stanford.edu/data/glove.6B.zip\n",
    "    - 폴더 구조\n",
    "        - data/glove.6B/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import config\n",
    "from dataloader.loader import Loader\n",
    "from preprocessing.utils import Preprocess, remove_empty_docs\n",
    "from dataloader.embeddings import GloVe\n",
    "from model.cnn_document_model import DocumentModel, TrainingParameters\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 학습 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델을 저장할 디렉토리 생성\n",
    "if not os.path.exists(os.path.join(config.MODEL_DIR, 'imdb')):\n",
    "    os.makedirs(os.path.join(config.MODEL_DIR, 'imdb'))\n",
    "\n",
    "# 학습 파라미터 설정\n",
    "train_params = TrainingParameters('imdb_transfer_tanh_activation', \n",
    "                                  model_file_path = config.MODEL_DIR+ '/imdb/transfer_model_10.hdf5',\n",
    "                                  model_hyper_parameters = config.MODEL_DIR+ '/imdb/transfer_model_10.json',\n",
    "                                  model_train_parameters = config.MODEL_DIR+ '/imdb/transfer_model_10_meta.json',\n",
    "                                  num_epochs=30,\n",
    "                                  batch_size=128)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yurikim/Desktop/study/transfer_learning/practice/02_text/preprocessing/utils.py:15: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape : (1250, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yurikim/Desktop/study/transfer_learning/practice/02_text/preprocessing/utils.py:15: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_df.shape : (25000, 2)\n",
      "corpus size : 1250\n",
      "target size : 1250\n"
     ]
    }
   ],
   "source": [
    "# 다운받은 IMDB 데이터 로드: 학습셋은 5%만 취한다 (전체는 2만5천개)\n",
    "train_df = Loader.load_imdb_data(directory = 'train')\n",
    "train_df = train_df.sample(frac=0.05, random_state = train_params.seed)\n",
    "print(f'train_df.shape : {train_df.shape}')\n",
    "\n",
    "test_df = Loader.load_imdb_data(directory = 'test')\n",
    "print(f'test_df.shape : {test_df.shape}')\n",
    "\n",
    "# 텍스트 데이터, 레이블 추출\n",
    "corpus = train_df['review'].tolist()\n",
    "target = train_df['sentiment'].tolist()\n",
    "corpus, target = remove_empty_docs(corpus, target)\n",
    "print(f'corpus size : {len(corpus)}')\n",
    "print(f'target size : {len(target)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## > 리뷰 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>A man and his wife get in a horrible car accid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3058</th>\n",
       "      <td>Well, what can I say, this movie really got to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12016</th>\n",
       "      <td>This early version of the tale 'The Student of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12239</th>\n",
       "      <td>To a certain extent, I actually liked this fil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21127</th>\n",
       "      <td>I watched this film, along with every other ad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24667</th>\n",
       "      <td>I'm a huge Steven Seagal fan. Hell, I probably...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14460</th>\n",
       "      <td>It's not a terrible movie, really, and Glenn a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12692</th>\n",
       "      <td>There is no doubt that this film has an impres...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6400</th>\n",
       "      <td>Most of the feedback I've heard concerning Mea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17333</th>\n",
       "      <td>I couldn't agree more with another reviewer th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1250 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "1152   A man and his wife get in a horrible car accid...          1\n",
       "3058   Well, what can I say, this movie really got to...          1\n",
       "12016  This early version of the tale 'The Student of...          1\n",
       "12239  To a certain extent, I actually liked this fil...          1\n",
       "21127  I watched this film, along with every other ad...          0\n",
       "...                                                  ...        ...\n",
       "24667  I'm a huge Steven Seagal fan. Hell, I probably...          0\n",
       "14460  It's not a terrible movie, really, and Glenn a...          0\n",
       "12692  There is no doubt that this film has an impres...          0\n",
       "6400   Most of the feedback I've heard concerning Mea...          1\n",
       "17333  I couldn't agree more with another reviewer th...          0\n",
       "\n",
       "[1250 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전처리 결과\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      ">> target: 0\n",
      ">> review\n",
      "Autobiography of founder of zoo in NYC starts out by being very cute and would be great family movie if it stayed there. however we get more and more involved with reality as gorilla grows up to be a wild thing not easily amenable to his \"mother's\" wishes - this might scare younger children, esp. scenes where Buddy tries to injure Gertrude. rather quick resolution at the end. below average.\n",
      "----------------------------------------------------------------------------------------------------\n",
      ">> target: 1\n",
      ">> review\n",
      "Last November, I had a chance to see this film at the Reno Film Festival. I have to say that it was a lot of fun. A few tech errors aside, it was a great experience. I loved the writing and acting, especially from the guy that played the lead role. There is a lot of heart in this movie, a lot of wit to. I got a chance to speak with a few of the filmmakers after it was done, and they seemed real nice. All in all the whole movie was just a positive experience, and one I'd definitely recommend. The story was entertaining and cool, as a woman I've been through a lot of the same problems as the lead guy, and I could really understand his problems because of it. The movie does a great job of giving us people we can sympathize with. The friends in the movie are really well written to, they are realistic. I know people like these, I only wish Imy friends and I could sound as cool as these people when we talk. The whole movie is just real cool, I wish there were more films out there like it.- Jayden\n",
      "----------------------------------------------------------------------------------------------------\n",
      ">> target: 1\n",
      ">> review\n",
      "Moon Child is the story of two brothers and a friend trying to make it in a futuristic, economically-unstable Japan. After a cunning disaster gone wrong, someone new enters young Sho's life, a special friend by the name of Kei. Years later they have grown rather close, and have found ways to combine both their talents into one unstoppable team. During another escapade, they encounter a new friend and his mute sister who become part of their band of friends. Before long disaster again strikes and the group falls apart. Alliances turn to enemies and their worlds are all turned upside down. Regrets and hopelessness claim some while power and success take others. Tragedy claims still others. Truths are revealed and lives are forever changed. And you will never see a more beautiful sunrise.This movie is a gripping tale of undying friendships, webs of relationships, and a team that not even death can keep apart for too long. Moon child combines sci-fi, drama, and action with the perfect cast and talent to create the most sensationally moving movie of the time, and great for most audiences. It minimizes the everyday romances and puts more emphasis on the important values we can all relate to such as friendships, loyalty, and believing in yourself. Nothing could possibly compare. I personally have never seen anything quite like it, and I don't suspect I ever will again.It appeals to the wider population in many ways and is a must see for all.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in random.sample(range(train_df.shape[0]),3):\n",
    "    print('-'*100)\n",
    "    print(f\">> target: {target[i]}\")\n",
    "    print('>> review')\n",
    "    print(corpus[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 전처리"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. 텍스트 -> 시퀀스 변환(인덱스 시퀀스)\n",
    "\n",
    "- 참고. 사용자 정의 함수에서 nltk 라이브러리의 wordpunct_tokenize를 사용함\n",
    "    - -> 아래 코드 필요 ... nltk.download('punkt')\n",
    "    - 참고. from preprocessing.utils import Preprocess, remove_empty_docs\n",
    "        - utils.py 내 from nltk.tokenize import sent_tokenize, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4990 unique tokens.\n",
      "All documents processed.cessed."
     ]
    }
   ],
   "source": [
    "# 학습셋을 인덱스 시퀀스로 변환\n",
    "preprocessor = Preprocess(corpus=corpus)\n",
    "corpus_to_seq = preprocessor.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1250"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_to_seq)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## > 시퀀스 변환 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A man and his wife get in a horrible car accident. When the wife is left in a persistent vegetative state, the man must choose between pulling the plug and letting her live. The decision is made even harder when he realizes her ghost wants to extract revenge on him and those around him.This comes to us from director Rob Schmidt, who made \"Wrong Turn\" (a film I have not seen). With only one horror film under his belt, and not a particularly notorious one at that, I was a bit reluctant to watch this episode, expecting Schmidt to be a \"Master of Horror\" in only the most liberal sense. My apologies to him for my underestimation. As of episode 10 in a 13 episode season, this was actually the best one yet.The issue of the \"right to die\" is dealt with and covered in enough detail to be a solid plot device. However, this is only the foundation on which the story revolves. Once the horror elements show up, the film goes from \"decent\" to \"spectacular\". Great acting, great plot, great dialogue, great suspense. I was a little creeped out at times (which is good) and most of all: the gore is in extreme abundance! I read a review of this episode prior to watching it, where the reviewer said there is a strong hint of \"Hellraiser\" in this. Through the first part of the show, I had no idea what they were talking about. Then there is a bit later where some images do remind me of \"Hellraiser 2\". However, I in no way wish to say that this takes away from the film. I can see no other way to create the effect that was created, and in my opinion this looks remarkably better than \"Hellraiser 2\".Some plot twists show up later on, and might invite the viewer to give the film a second look. I didn\\'t watch it a second time, but I think the beginning would make more sense if I had (not that it\\'s confusing). The subplot with the dental hygienist is also nice, and I found myself going back and forth about whether I disliked the main character for his relationship with her or if I felt bad for him. He\\'s somewhat of an anti-hero to the whole story, if you will. I feel inclined to cheer for him as the protagonist, but he\\'s completely unlovable.While the Stuart Gordon episode may be better and I\\'m excited about the \"Washingtonians\" episode, I think I could safely bet that this is the key episode of the season and by far the saving grace of what was otherwise lackluster and routine. When legends like John Carpenter let me down (again) I get a bit worried about the genre\\'s future, but then a fresh face like Rob Schmidt comes along and gives me hope. This one is a keeper, and please bring Schmidt back for season 3!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2,   3,   4,   5,   6,   7,   8,   2,   9,  10,  11,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,  12,  13,   6,  14,  15,   8,   2,  16,  13,\n",
       "         3,  17,  18,  19,  13,   4,  20,  21,  22,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  13,  23,  14,  24,  25,\n",
       "        26,  12,  27,  28,  21,  29,  30,  31,  32,  33,  34,   4,  35,\n",
       "        36,  34,  37,  38,  31,  39,  40,  41,  42,  43,  24,  44,  51,\n",
       "        52,  53,  54,  46,  55,   5,  56,   4,  49,   2,  57,  58,  53,\n",
       "        59,  60,  47,  61,   2,  62,  31,  63,  37,  64,  65,  31,  66,\n",
       "         2,  67,  68,  72,  31,  34,  73,  72,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  74,  68,  64,   8,   2,  64,\n",
       "        75,  37,  61,  76,  13,  77,  53,  78,  13,  79,  68,  13,  80,\n",
       "        31,  81,  14,  82,  51,   4,  83,   8,  84,  85,  31,  89,  37,\n",
       "        14,  52,  13,  33,  90,  13,  91,  92,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  93,  13,  54,  94,  95,  96,  13,  46,  97,  40,  98,\n",
       "        31,  99,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0, 100, 101, 100,  87, 100, 102, 100,\n",
       "       103,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  47,  61,   2,\n",
       "       104, 105,  59, 106,  90,  14, 107,   4,  69,  68, 108,  13, 109,\n",
       "        14,   8, 110,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_to_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All documents processed.ocessed."
     ]
    }
   ],
   "source": [
    "# 테스트셋을 인덱스 시퀀스로 변환\n",
    "test_corpus = test_df['review'].tolist()\n",
    "test_target = test_df['sentiment'].tolist()\n",
    "test_corpus, test_target = remove_empty_docs(test_corpus, test_target)\n",
    "test_corpus_to_seq = preprocessor.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_corpus_to_seq size : 25000\n",
      "test_corpus_to_seq[0] size : 300\n"
     ]
    }
   ],
   "source": [
    "print(f'test_corpus_to_seq size : {len(test_corpus_to_seq)}')\n",
    "print(f'test_corpus_to_seq[0] size : {len(test_corpus_to_seq[0])}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2. 학습셋 & 테스트셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape : (1250, 300)\n",
      "y_train.shape : (1250,)\n",
      "x_test.shape : (25000, 300)\n",
      "y_test.shape : (25000,)\n"
     ]
    }
   ],
   "source": [
    "# 학습셋, 테스트셋 준비\n",
    "x_train = np.array(corpus_to_seq)\n",
    "x_test = np.array(test_corpus_to_seq)\n",
    "y_train = np.array(target)\n",
    "y_test = np.array(test_target)\n",
    "\n",
    "print(f'x_train.shape : {x_train.shape}')\n",
    "print(f'y_train.shape : {y_train.shape}')\n",
    "print(f'x_test.shape : {x_test.shape}')\n",
    "print(f'y_test.shape : {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2,   3,   4, ...,   0,   0,   0],\n",
       "       [229, 129, 146, ...,   0,   0,   0],\n",
       "       [ 37, 272, 273, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [119,  14, 127, ...,   0,   0,   0],\n",
       "       [ 69,  68,  13, ...,   0,   0,   0],\n",
       "       [ 47, 581, 163, ...,   0,   0,   0]], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. word embedding - GloVe 방법"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사전 훈련된 Word2Vec 임베딩 불러오기(GloVe)\n",
    "    - 다운로드 링크 : http://nlp.stanford.edu/data/glove.6B.zip\n",
    "- 참고. 사용자 정의함수로 GloVe 함수 만듦\n",
    "    - from dataloader.embeddings import GloVe\n",
    "    \n",
    "        ```python\n",
    "        class GloVe:\n",
    "            def __init__(self, embd_dim=50):\n",
    "                if embd_dim not in [50, 100, 200, 300]:\n",
    "                    raise ValueError('embedding dim should be one of [50, 100, 200, 300]')\n",
    "                self.EMBEDDING_DIM = embd_dim\n",
    "                self.embedding_matrix = None\n",
    "                \n",
    "            def __load__(self):\n",
    "                print('Reading {} dim GloVe vectors'.format(self.EMBEDDING_DIM))\n",
    "                self.embeddings_index = {}\n",
    "                # 사전학습 임베딩 가져오기 (data/glove.6B/glove.6B.50d.txt)\n",
    "                # ex. 단어 of의 경우: of 0.70853 0.57088 -0.4716 0.18048 ...\n",
    "                # embeddings_index 딕셔너리에 {\"of\":[0.70853, 0.57088, -0.4716, 0.18048, ...]}로 전처리\n",
    "                with open(os.path.join(config.GLOVE_DIR, 'glove.6B.'+str(self.EMBEDDING_DIM)+'d.txt'),encoding=\"utf8\") as fin:\n",
    "                    for line in fin:\n",
    "                        try:\n",
    "                            values = line.split()\n",
    "                            coefs = np.asarray(values[1:], dtype='float32')\n",
    "                            word = values[0]\n",
    "                            self.embeddings_index[word] = coefs\n",
    "                        except:\n",
    "                            print(line)\n",
    "\n",
    "                print('Found %s word vectors.' % len(self.embeddings_index))\n",
    "\n",
    "            def _init_embedding_matrix(self, word_index_dict, oov_words_file='OOV-Words.txt'):\n",
    "                # 임베딩 채울 영행렬\n",
    "                self.embedding_matrix = np.zeros((len(word_index_dict)+2 , self.EMBEDDING_DIM)) # +1 for the 0 word index from paddings.\n",
    "                not_found_words=0\n",
    "                missing_word_index = []\n",
    "                \n",
    "                with open(oov_words_file, 'w') as f: \n",
    "                    for word, i in word_index_dict.items():\n",
    "                        # embeddings_index: glove.6B.50d.txt 사전학습 임베딩 전처리 딕셔너리\n",
    "                        embedding_vector = self.embeddings_index.get(word) \n",
    "                        if embedding_vector is not None:\n",
    "                            # words not found in embedding index will be all-zeros.\n",
    "                            self.embedding_matrix[i] = embedding_vector\n",
    "                        else:\n",
    "                            not_found_words+=1\n",
    "                            f.write(word + ','+str(i)+'\\n')\n",
    "                            missing_word_index.append(i)\n",
    "\n",
    "                    #oov by average vector:\n",
    "                    self.embedding_matrix[1] = np.mean(self.embedding_matrix, axis=0)\n",
    "                    for indx in missing_word_index:\n",
    "                        self.embedding_matrix[indx] = np.random.rand(self.EMBEDDING_DIM)+ self.embedding_matrix[1]\n",
    "                print(\"words not found in embeddings: {}\".format(not_found_words))\n",
    "                \n",
    "                \n",
    "            def get_embedding(self, word_index_dict): # input: 단어와 인덱스 딕셔너리\n",
    "                if self.embedding_matrix is None:\n",
    "                    self._load()\n",
    "                    self._init_embedding_matrix(word_index_dict) \n",
    "                return self.embedding_matrix\n",
    "        ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 2,\n",
       " 'man': 3,\n",
       " 'and': 4,\n",
       " 'his': 5,\n",
       " 'wife': 6,\n",
       " 'get': 7,\n",
       " 'in': 8,\n",
       " 'horrible': 9,\n",
       " 'car': 10,\n",
       " 'accident': 11,\n",
       " 'when': 12,\n",
       " 'the': 13,\n",
       " 'is': 14,\n",
       " 'left': 15,\n",
       " 'state': 16,\n",
       " 'must': 17,\n",
       " 'choose': 18,\n",
       " 'between': 19,\n",
       " 'letting': 20,\n",
       " 'her': 21,\n",
       " 'live': 22,\n",
       " 'decision': 23,\n",
       " 'made': 24,\n",
       " 'even': 25,\n",
       " 'harder': 26,\n",
       " 'he': 27,\n",
       " 'realizes': 28,\n",
       " 'ghost': 29,\n",
       " 'wants': 30,\n",
       " 'to': 31,\n",
       " 'revenge': 32,\n",
       " 'on': 33,\n",
       " 'him': 34,\n",
       " 'those': 35,\n",
       " 'around': 36,\n",
       " 'this': 37,\n",
       " 'comes': 38,\n",
       " 'us': 39,\n",
       " 'from': 40,\n",
       " 'director': 41,\n",
       " 'rob': 42,\n",
       " 'who': 43,\n",
       " 'wrong': 44,\n",
       " 'turn': 45,\n",
       " 'film': 46,\n",
       " 'i': 47,\n",
       " 'have': 48,\n",
       " 'not': 49,\n",
       " 'seen': 50,\n",
       " 'with': 51,\n",
       " 'only': 52,\n",
       " 'one': 53,\n",
       " 'horror': 54,\n",
       " 'under': 55,\n",
       " 'belt': 56,\n",
       " 'particularly': 57,\n",
       " 'notorious': 58,\n",
       " 'at': 59,\n",
       " 'that': 60,\n",
       " 'was': 61,\n",
       " 'bit': 62,\n",
       " 'watch': 63,\n",
       " 'episode': 64,\n",
       " 'expecting': 65,\n",
       " 'be': 66,\n",
       " 'master': 67,\n",
       " 'of': 68,\n",
       " 'most': 69,\n",
       " 'liberal': 70,\n",
       " 'sense': 71,\n",
       " 'my': 72,\n",
       " 'for': 73,\n",
       " 'as': 74,\n",
       " 'season': 75,\n",
       " 'actually': 76,\n",
       " 'best': 77,\n",
       " 'yet': 78,\n",
       " 'issue': 79,\n",
       " 'right': 80,\n",
       " 'die': 81,\n",
       " 'dealt': 82,\n",
       " 'covered': 83,\n",
       " 'enough': 84,\n",
       " 'detail': 85,\n",
       " 'solid': 86,\n",
       " 'plot': 87,\n",
       " 'device': 88,\n",
       " 'however': 89,\n",
       " 'which': 90,\n",
       " 'story': 91,\n",
       " 'revolves': 92,\n",
       " 'once': 93,\n",
       " 'elements': 94,\n",
       " 'show': 95,\n",
       " 'up': 96,\n",
       " 'goes': 97,\n",
       " 'decent': 98,\n",
       " 'spectacular': 99,\n",
       " 'great': 100,\n",
       " 'acting': 101,\n",
       " 'dialogue': 102,\n",
       " 'suspense': 103,\n",
       " 'little': 104,\n",
       " 'out': 105,\n",
       " 'times': 106,\n",
       " 'good': 107,\n",
       " 'all': 108,\n",
       " 'gore': 109,\n",
       " 'extreme': 110,\n",
       " 'read': 111,\n",
       " 'review': 112,\n",
       " 'prior': 113,\n",
       " 'watching': 114,\n",
       " 'it': 115,\n",
       " 'where': 116,\n",
       " 'reviewer': 117,\n",
       " 'said': 118,\n",
       " 'there': 119,\n",
       " 'strong': 120,\n",
       " 'hint': 121,\n",
       " 'hellraiser': 122,\n",
       " 'through': 123,\n",
       " 'first': 124,\n",
       " 'part': 125,\n",
       " 'had': 126,\n",
       " 'no': 127,\n",
       " 'idea': 128,\n",
       " 'what': 129,\n",
       " 'they': 130,\n",
       " 'were': 131,\n",
       " 'talking': 132,\n",
       " 'about': 133,\n",
       " 'then': 134,\n",
       " 'later': 135,\n",
       " 'some': 136,\n",
       " 'images': 137,\n",
       " 'do': 138,\n",
       " 'remind': 139,\n",
       " 'me': 140,\n",
       " 'way': 141,\n",
       " 'wish': 142,\n",
       " 'say': 143,\n",
       " 'takes': 144,\n",
       " 'away': 145,\n",
       " 'can': 146,\n",
       " 'see': 147,\n",
       " 'other': 148,\n",
       " 'create': 149,\n",
       " 'effect': 150,\n",
       " 'created': 151,\n",
       " 'opinion': 152,\n",
       " 'looks': 153,\n",
       " 'better': 154,\n",
       " 'than': 155,\n",
       " 'twists': 156,\n",
       " 'might': 157,\n",
       " 'viewer': 158,\n",
       " 'give': 159,\n",
       " 'second': 160,\n",
       " 'look': 161,\n",
       " 'didn': 162,\n",
       " 't': 163,\n",
       " 'time': 164,\n",
       " 'but': 165,\n",
       " 'think': 166,\n",
       " 'beginning': 167,\n",
       " 'would': 168,\n",
       " 'make': 169,\n",
       " 'more': 170,\n",
       " 'if': 171,\n",
       " 's': 172,\n",
       " 'confusing': 173,\n",
       " 'also': 174,\n",
       " 'nice': 175,\n",
       " 'found': 176,\n",
       " 'myself': 177,\n",
       " 'going': 178,\n",
       " 'back': 179,\n",
       " 'forth': 180,\n",
       " 'whether': 181,\n",
       " 'main': 182,\n",
       " 'character': 183,\n",
       " 'relationship': 184,\n",
       " 'or': 185,\n",
       " 'felt': 186,\n",
       " 'bad': 187,\n",
       " 'somewhat': 188,\n",
       " 'an': 189,\n",
       " 'anti': 190,\n",
       " 'hero': 191,\n",
       " 'whole': 192,\n",
       " 'you': 193,\n",
       " 'will': 194,\n",
       " 'feel': 195,\n",
       " 'protagonist': 196,\n",
       " 'completely': 197,\n",
       " 'while': 198,\n",
       " 'stuart': 199,\n",
       " 'gordon': 200,\n",
       " 'may': 201,\n",
       " 'm': 202,\n",
       " 'excited': 203,\n",
       " 'could': 204,\n",
       " 'bet': 205,\n",
       " 'key': 206,\n",
       " 'by': 207,\n",
       " 'far': 208,\n",
       " 'saving': 209,\n",
       " 'grace': 210,\n",
       " 'otherwise': 211,\n",
       " 'lackluster': 212,\n",
       " 'routine': 213,\n",
       " 'like': 214,\n",
       " 'john': 215,\n",
       " 'let': 216,\n",
       " 'down': 217,\n",
       " 'again': 218,\n",
       " 'worried': 219,\n",
       " 'genre': 220,\n",
       " 'future': 221,\n",
       " 'fresh': 222,\n",
       " 'face': 223,\n",
       " 'along': 224,\n",
       " 'gives': 225,\n",
       " 'hope': 226,\n",
       " 'please': 227,\n",
       " 'bring': 228,\n",
       " 'well': 229,\n",
       " 'movie': 230,\n",
       " 'really': 231,\n",
       " 'got': 232,\n",
       " 'so': 233,\n",
       " 'many': 234,\n",
       " 'loved': 235,\n",
       " 'although': 236,\n",
       " 'seems': 237,\n",
       " 'simple': 238,\n",
       " 'rather': 239,\n",
       " 'boring': 240,\n",
       " 'isn': 241,\n",
       " 'enjoyed': 242,\n",
       " 'soundtrack': 243,\n",
       " 'adams': 244,\n",
       " 'drama': 245,\n",
       " 'spirit': 246,\n",
       " 'gets': 247,\n",
       " 'your': 248,\n",
       " 'attention': 249,\n",
       " 'thing': 250,\n",
       " 'human': 251,\n",
       " 'voice': 252,\n",
       " 'interact': 253,\n",
       " 'makes': 254,\n",
       " 'realistic': 255,\n",
       " 'animations': 256,\n",
       " 'seem': 257,\n",
       " 'now': 258,\n",
       " 'don': 259,\n",
       " 'know': 260,\n",
       " 'making': 261,\n",
       " 'animals': 262,\n",
       " 'talk': 263,\n",
       " 'just': 264,\n",
       " 'lame': 265,\n",
       " 'beautiful': 266,\n",
       " 'recommend': 267,\n",
       " 'everyone': 268,\n",
       " 'kids': 269,\n",
       " 'because': 270,\n",
       " 'very': 271,\n",
       " 'early': 272,\n",
       " 'version': 273,\n",
       " 'tale': 274,\n",
       " 'student': 275,\n",
       " 'germany': 276,\n",
       " 'starring': 277,\n",
       " 'paul': 278,\n",
       " 'few': 279,\n",
       " 'years': 280,\n",
       " 'plays': 281,\n",
       " 'role': 282,\n",
       " 'technically': 283,\n",
       " 'impressive': 284,\n",
       " 'year': 285,\n",
       " 'old': 286,\n",
       " 'them': 287,\n",
       " 'same': 288,\n",
       " 'shot': 289,\n",
       " 'after': 290,\n",
       " 'meeting': 291,\n",
       " 'mysterious': 292,\n",
       " 'gold': 293,\n",
       " 'needs': 294,\n",
       " 'woo': 295,\n",
       " 'previously': 296,\n",
       " 'saved': 297,\n",
       " 'moving': 298,\n",
       " 'fast': 299,\n",
       " 'pace': 300,\n",
       " 'runs': 301,\n",
       " 'over': 302,\n",
       " 'hour': 303,\n",
       " 'fairly': 304,\n",
       " 'written': 305,\n",
       " 'has': 306,\n",
       " 'legend': 307,\n",
       " 'dr': 308,\n",
       " 'mr': 309,\n",
       " 'hyde': 310,\n",
       " 'starting': 311,\n",
       " 'does': 312,\n",
       " 'figure': 313,\n",
       " 'potential': 314,\n",
       " 'evil': 315,\n",
       " 'developing': 316,\n",
       " 'into': 317,\n",
       " 'sides': 318,\n",
       " 'person': 319,\n",
       " 'certain': 320,\n",
       " 'extent': 321,\n",
       " 'liked': 322,\n",
       " 'original': 323,\n",
       " 'vampires': 324,\n",
       " 'quite': 325,\n",
       " 'woman': 326,\n",
       " 'fan': 327,\n",
       " 'used': 328,\n",
       " 'fact': 329,\n",
       " 'women': 330,\n",
       " 'are': 331,\n",
       " 'slap': 332,\n",
       " 'lee': 333,\n",
       " 'too': 334,\n",
       " 'much': 335,\n",
       " 'fully': 336,\n",
       " 'realized': 337,\n",
       " 'guy': 338,\n",
       " 'played': 339,\n",
       " 'sidekick': 340,\n",
       " 'deadly': 341,\n",
       " 'jon': 342,\n",
       " 'okay': 343,\n",
       " 'yeah': 344,\n",
       " 'actor': 345,\n",
       " 'ok': 346,\n",
       " 'least': 347,\n",
       " 'doesn': 348,\n",
       " 'start': 349,\n",
       " 'sing': 350,\n",
       " 'catch': 351,\n",
       " 'cable': 352,\n",
       " 'action': 353,\n",
       " 'month': 354,\n",
       " 'watched': 355,\n",
       " 'every': 356,\n",
       " 'adaptation': 357,\n",
       " 'hands': 358,\n",
       " 'including': 359,\n",
       " 'seeing': 360,\n",
       " 'research': 361,\n",
       " 'cinematography': 362,\n",
       " 'music': 363,\n",
       " 'unfortunately': 364,\n",
       " 'life': 365,\n",
       " 'taken': 366,\n",
       " 'never': 367,\n",
       " 'such': 368,\n",
       " 'awful': 369,\n",
       " 'portrayal': 370,\n",
       " 'rochester': 371,\n",
       " 'gone': 372,\n",
       " 'wit': 373,\n",
       " 'passion': 374,\n",
       " 'scott': 375,\n",
       " 'closely': 376,\n",
       " 'st': 377,\n",
       " 'novel': 378,\n",
       " 'playing': 379,\n",
       " 'passionate': 380,\n",
       " 'content': 381,\n",
       " 'things': 382,\n",
       " 'thinking': 383,\n",
       " 'slightly': 384,\n",
       " 'higher': 385,\n",
       " 'vote': 386,\n",
       " 'based': 387,\n",
       " 'wonderful': 388,\n",
       " 'honestly': 389,\n",
       " 'bear': 390,\n",
       " 'long': 391,\n",
       " 'george': 392,\n",
       " 'c': 393,\n",
       " 'performance': 394,\n",
       " 'biggest': 395,\n",
       " 'waste': 396,\n",
       " 'nine': 397,\n",
       " 'dollars': 398,\n",
       " 've': 399,\n",
       " 'spent': 400,\n",
       " 'knew': 401,\n",
       " 'how': 402,\n",
       " 'often': 403,\n",
       " 'went': 404,\n",
       " 'movies': 405,\n",
       " 'd': 406,\n",
       " 'probably': 407,\n",
       " 'hard': 408,\n",
       " 'imagine': 409,\n",
       " 'less': 410,\n",
       " 'true': 411,\n",
       " 'trailer': 412,\n",
       " 're': 413,\n",
       " 'mystery': 414,\n",
       " 'why': 415,\n",
       " 'wouldn': 416,\n",
       " 'nothing': 417,\n",
       " 'intriguing': 418,\n",
       " 'exciting': 419,\n",
       " 'none': 420,\n",
       " 'these': 421,\n",
       " 'script': 422,\n",
       " 'complete': 423,\n",
       " 'flop': 424,\n",
       " 'reading': 425,\n",
       " 'planning': 426,\n",
       " 'go': 427,\n",
       " 'anything': 428,\n",
       " 'forty': 429,\n",
       " 'five': 430,\n",
       " 'minutes': 431,\n",
       " 'afraid': 432,\n",
       " 'disappointment': 433,\n",
       " 'asking': 434,\n",
       " 'yourself': 435,\n",
       " 'tell': 436,\n",
       " 'neither': 437,\n",
       " 'nor': 438,\n",
       " 'suspenseful': 439,\n",
       " 'edge': 440,\n",
       " 'frightened': 441,\n",
       " 'curious': 442,\n",
       " 'laughable': 443,\n",
       " 'numerous': 444,\n",
       " 'throughout': 445,\n",
       " 'ridiculous': 446,\n",
       " 'began': 447,\n",
       " 'write': 448,\n",
       " 'off': 449,\n",
       " 'comic': 450,\n",
       " 'relief': 451,\n",
       " 'find': 452,\n",
       " 'seconds': 453,\n",
       " 'wasn': 454,\n",
       " 'absolutely': 455,\n",
       " 'dreadful': 456,\n",
       " 'cage': 457,\n",
       " 'miss': 458,\n",
       " 'without': 459,\n",
       " 'exception': 460,\n",
       " 'incredibly': 461,\n",
       " 'below': 462,\n",
       " 'average': 463,\n",
       " 'moment': 464,\n",
       " 'finally': 465,\n",
       " 'end': 466,\n",
       " 'coming': 467,\n",
       " 'mile': 468,\n",
       " 'am': 469,\n",
       " 'usually': 470,\n",
       " 'harsh': 471,\n",
       " 'critic': 472,\n",
       " 'frankly': 473,\n",
       " 'comedy': 474,\n",
       " 'want': 475,\n",
       " 'laugh': 476,\n",
       " 'surprised': 477,\n",
       " 'poorly': 478,\n",
       " 'acted': 479,\n",
       " 'overwhelming': 480,\n",
       " 'favor': 481,\n",
       " 'something': 482,\n",
       " 'else': 483,\n",
       " 'celebration': 484,\n",
       " 'earth': 485,\n",
       " 'day': 486,\n",
       " 'disney': 487,\n",
       " 'released': 488,\n",
       " 'short': 489,\n",
       " 'any': 490,\n",
       " 'message': 491,\n",
       " 'we': 492,\n",
       " 'treated': 493,\n",
       " 'excellent': 494,\n",
       " 'footage': 495,\n",
       " 'their': 496,\n",
       " 'feeling': 497,\n",
       " 'ourselves': 498,\n",
       " 'stars': 499,\n",
       " 'herd': 500,\n",
       " 'family': 501,\n",
       " 'polar': 502,\n",
       " 'bears': 503,\n",
       " 'whale': 504,\n",
       " 'its': 505,\n",
       " 'narrative': 506,\n",
       " 'begins': 507,\n",
       " 'north': 508,\n",
       " 'proceeds': 509,\n",
       " 'south': 510,\n",
       " 'until': 511,\n",
       " 'reach': 512,\n",
       " 'being': 513,\n",
       " 'introduced': 514,\n",
       " 'various': 515,\n",
       " 'mentioned': 516,\n",
       " 'view': 517,\n",
       " 'note': 518,\n",
       " 'sea': 519,\n",
       " 'ice': 520,\n",
       " 'recent': 521,\n",
       " 'father': 522,\n",
       " 'desperate': 523,\n",
       " 'search': 524,\n",
       " 'food': 525,\n",
       " 'leads': 526,\n",
       " 'dangerous': 527,\n",
       " 'solution': 528,\n",
       " 'shots': 529,\n",
       " 'across': 530,\n",
       " 'ever': 531,\n",
       " 'saw': 532,\n",
       " 'another': 533,\n",
       " 'price': 534,\n",
       " 'big': 535,\n",
       " 'screen': 536,\n",
       " 'terrific': 537,\n",
       " 'white': 538,\n",
       " 'sharks': 539,\n",
       " 'taking': 540,\n",
       " 'filmed': 541,\n",
       " 'slow': 542,\n",
       " 'motion': 543,\n",
       " 'wild': 544,\n",
       " 'incredible': 545,\n",
       " 'speed': 546,\n",
       " 'convey': 547,\n",
       " 'recall': 548,\n",
       " 'television': 549,\n",
       " 'kung': 550,\n",
       " 'fu': 551,\n",
       " 'during': 552,\n",
       " 'interesting': 553,\n",
       " 'credits': 554,\n",
       " 'roll': 555,\n",
       " 'techniques': 556,\n",
       " 'revealed': 557,\n",
       " 'dramatic': 558,\n",
       " 'humorous': 559,\n",
       " 'moments': 560,\n",
       " 'choice': 561,\n",
       " 'nature': 562,\n",
       " 'perhaps': 563,\n",
       " 'editing': 564,\n",
       " 'prey': 565,\n",
       " 'been': 566,\n",
       " 'available': 567,\n",
       " 'dvd': 568,\n",
       " 'own': 569,\n",
       " 'take': 570,\n",
       " 'three': 571,\n",
       " 'lost': 572,\n",
       " 'writers': 573,\n",
       " 'directors': 574,\n",
       " 'line': 575,\n",
       " 'special': 576,\n",
       " 'seemed': 577,\n",
       " 'focused': 578,\n",
       " 'amusing': 579,\n",
       " 'children': 580,\n",
       " 'couldn': 581,\n",
       " 'accomplish': 582,\n",
       " 'small': 583,\n",
       " 'low': 584,\n",
       " 'budget': 585,\n",
       " 'films': 586,\n",
       " 'become': 587,\n",
       " 'near': 588,\n",
       " 'fortunately': 589,\n",
       " 'easily': 590,\n",
       " 'forgotten': 591,\n",
       " 'chance': 592,\n",
       " 'actors': 593,\n",
       " 'money': 594,\n",
       " 'side': 595,\n",
       " 'came': 596,\n",
       " 'real': 597,\n",
       " 'anyone': 598,\n",
       " 'shred': 599,\n",
       " 'respect': 600,\n",
       " 'should': 601,\n",
       " 'avoid': 602,\n",
       " 'costs': 603,\n",
       " 'describe': 604,\n",
       " 'high': 605,\n",
       " 'list': 606,\n",
       " 'favourite': 607,\n",
       " 'ended': 608,\n",
       " 'entirely': 609,\n",
       " 'visual': 610,\n",
       " 'style': 611,\n",
       " 'examples': 612,\n",
       " 'cinema': 613,\n",
       " 'shooting': 614,\n",
       " 'perfectly': 615,\n",
       " 'image': 616,\n",
       " 'sound': 617,\n",
       " 'effects': 618,\n",
       " 'props': 619,\n",
       " 'lighting': 620,\n",
       " 'etc': 621,\n",
       " 'equal': 622,\n",
       " 'despite': 623,\n",
       " 'hollywood': 624,\n",
       " 'works': 625,\n",
       " 'order': 626,\n",
       " 'combination': 627,\n",
       " 'presented': 628,\n",
       " 'communicate': 629,\n",
       " 'contribution': 630,\n",
       " 'element': 631,\n",
       " 'meaning': 632,\n",
       " 'wrote': 633,\n",
       " 'glowing': 634,\n",
       " 'misguided': 635,\n",
       " 'project': 636,\n",
       " 'related': 637,\n",
       " 'writer': 638,\n",
       " 'star': 639,\n",
       " 'appealing': 640,\n",
       " 'connected': 641,\n",
       " 'tightly': 642,\n",
       " 'inner': 643,\n",
       " 'entertain': 644,\n",
       " 'draw': 645,\n",
       " 'characters': 646,\n",
       " 'sure': 647,\n",
       " 'commercial': 648,\n",
       " 'designed': 649,\n",
       " 'mass': 650,\n",
       " 'appeal': 651,\n",
       " 'artistic': 652,\n",
       " 'expression': 653,\n",
       " 'audience': 654,\n",
       " 'struggle': 655,\n",
       " 'grasp': 656,\n",
       " 'moved': 657,\n",
       " 'psychological': 658,\n",
       " 'easier': 659,\n",
       " 'buy': 660,\n",
       " 'sadly': 661,\n",
       " 'british': 662,\n",
       " 'since': 663,\n",
       " 'though': 664,\n",
       " 'cover': 665,\n",
       " 'claims': 666,\n",
       " 'buying': 667,\n",
       " 'homage': 668,\n",
       " 'horrors': 669,\n",
       " 'cradle': 670,\n",
       " 'mini': 671,\n",
       " 'tv': 672,\n",
       " 'done': 673,\n",
       " 'scene': 674,\n",
       " 'set': 675,\n",
       " 'lit': 676,\n",
       " 'exactly': 677,\n",
       " 'standard': 678,\n",
       " 'fill': 679,\n",
       " 'cheap': 680,\n",
       " 'finished': 681,\n",
       " 'piece': 682,\n",
       " 'opening': 683,\n",
       " 'obviously': 684,\n",
       " 'head': 685,\n",
       " 'torn': 686,\n",
       " 'apart': 687,\n",
       " 'tacky': 688,\n",
       " 'creature': 689,\n",
       " 'terrible': 690,\n",
       " 'cg': 691,\n",
       " 'impressed': 692,\n",
       " 'detective': 693,\n",
       " 'via': 694,\n",
       " 'filth': 695,\n",
       " 'convincing': 696,\n",
       " 'witnessed': 697,\n",
       " 'each': 698,\n",
       " 'last': 699,\n",
       " 'sets': 700,\n",
       " 'worse': 701,\n",
       " 'mental': 702,\n",
       " 'semi': 703,\n",
       " 'house': 704,\n",
       " 'sign': 705,\n",
       " 'outside': 706,\n",
       " 'bizarre': 707,\n",
       " 'cell': 708,\n",
       " 'took': 709,\n",
       " 'four': 710,\n",
       " 'attempts': 711,\n",
       " 'running': 712,\n",
       " 'opportunity': 713,\n",
       " 'point': 714,\n",
       " 'mark': 715,\n",
       " 'known': 716,\n",
       " 'appearances': 717,\n",
       " 'breasts': 718,\n",
       " 'guess': 719,\n",
       " 'um': 720,\n",
       " 'two': 721,\n",
       " 'downhill': 722,\n",
       " 'heard': 723,\n",
       " 'people': 724,\n",
       " 'alex': 725,\n",
       " 'love': 726,\n",
       " 'believe': 727,\n",
       " 'evidence': 728,\n",
       " 'likely': 729,\n",
       " 'league': 730,\n",
       " 'gentlemen': 731,\n",
       " 'christmas': 732,\n",
       " 'instead': 733,\n",
       " 'currently': 734,\n",
       " 'blair': 735,\n",
       " 'witch': 736,\n",
       " 'book': 737,\n",
       " 'shadows': 738,\n",
       " 'title': 739,\n",
       " 'worst': 740,\n",
       " 'hitchcock': 741,\n",
       " 'audiences': 742,\n",
       " 'aren': 743,\n",
       " 'interested': 744,\n",
       " 'puts': 745,\n",
       " 'protagonists': 746,\n",
       " 'danger': 747,\n",
       " 'need': 748,\n",
       " 'escape': 749,\n",
       " 'proves': 750,\n",
       " 'correct': 751,\n",
       " 'police': 752,\n",
       " 'jean': 753,\n",
       " 'guilty': 754,\n",
       " 'crime': 755,\n",
       " 'she': 756,\n",
       " 'howard': 757,\n",
       " 'decides': 758,\n",
       " 'course': 759,\n",
       " 'run': 760,\n",
       " 'body': 761,\n",
       " 'our': 762,\n",
       " 'charismatic': 763,\n",
       " 'pair': 764,\n",
       " 'jumping': 765,\n",
       " 'rocks': 766,\n",
       " 'top': 767,\n",
       " 'stuff': 768,\n",
       " 'mind': 769,\n",
       " 'unnecessary': 770,\n",
       " 'stayed': 771,\n",
       " 'put': 772,\n",
       " 'words': 773,\n",
       " 'figured': 774,\n",
       " 'cops': 775,\n",
       " 'newspaper': 776,\n",
       " 'death': 777,\n",
       " 'send': 778,\n",
       " 'type': 779,\n",
       " 'especially': 780,\n",
       " 'named': 781,\n",
       " 'cast': 782,\n",
       " 'morgan': 783,\n",
       " 'kevin': 784,\n",
       " 'l': 785,\n",
       " 'cool': 786,\n",
       " 'j': 787,\n",
       " 'cary': 788,\n",
       " 'above': 789,\n",
       " 'brings': 790,\n",
       " 'violent': 791,\n",
       " 'watchable': 792,\n",
       " 'young': 793,\n",
       " 'reporter': 794,\n",
       " 'throw': 795,\n",
       " 'edited': 796,\n",
       " 'others': 797,\n",
       " 'either': 798,\n",
       " 'guys': 799,\n",
       " 'ladies': 800,\n",
       " 'girl': 801,\n",
       " 'friends': 802,\n",
       " 'll': 803,\n",
       " 'whats': 804,\n",
       " 'required': 805,\n",
       " 'killings': 806,\n",
       " 'means': 807,\n",
       " 'definitely': 808,\n",
       " 'ratings': 809,\n",
       " 'points': 810,\n",
       " 'imdb': 811,\n",
       " 'christian': 812,\n",
       " 'needless': 813,\n",
       " 'scared': 814,\n",
       " 'scary': 815,\n",
       " 'anyway': 816,\n",
       " 'pretty': 817,\n",
       " 'cheesy': 818,\n",
       " 'considering': 819,\n",
       " 'remember': 820,\n",
       " 'still': 821,\n",
       " 'song': 822,\n",
       " 'ready': 823,\n",
       " 'ends': 824,\n",
       " 'behind': 825,\n",
       " 'suggest': 826,\n",
       " 'nostalgic': 827,\n",
       " 'reason': 828,\n",
       " 'besides': 829,\n",
       " 'remake': 830,\n",
       " 'cars': 831,\n",
       " 'hilarious': 832,\n",
       " 'number': 833,\n",
       " 'pressing': 834,\n",
       " 'questions': 835,\n",
       " 'firstly': 836,\n",
       " 'jennifer': 837,\n",
       " 'managed': 838,\n",
       " 'career': 839,\n",
       " 'limited': 840,\n",
       " 'range': 841,\n",
       " 'hammy': 842,\n",
       " 'facial': 843,\n",
       " 'expressions': 844,\n",
       " 'secondly': 845,\n",
       " 'responsible': 846,\n",
       " 'offensive': 847,\n",
       " 'deeply': 848,\n",
       " 'given': 849,\n",
       " 'decide': 850,\n",
       " 'shouldn': 851,\n",
       " 'someone': 852,\n",
       " 'system': 853,\n",
       " 'studio': 854,\n",
       " 'somewhere': 855,\n",
       " 'getting': 856,\n",
       " 'nasty': 857,\n",
       " 'billed': 858,\n",
       " 'certainly': 859,\n",
       " 'kind': 860,\n",
       " 'madness': 861,\n",
       " 'violence': 862,\n",
       " 'abuse': 863,\n",
       " 'essentially': 864,\n",
       " 'redeeming': 865,\n",
       " 'features': 866,\n",
       " 'anybody': 867,\n",
       " 'positive': 868,\n",
       " 'kim': 869,\n",
       " 'della': 870,\n",
       " 'twin': 871,\n",
       " 'luke': 872,\n",
       " 'jerk': 873,\n",
       " 'husband': 874,\n",
       " 'kenneth': 875,\n",
       " 'craig': 876,\n",
       " 'opens': 877,\n",
       " 'eve': 878,\n",
       " 'home': 879,\n",
       " 'work': 880,\n",
       " 'driving': 881,\n",
       " 'add': 882,\n",
       " 'business': 883,\n",
       " 'partner': 884,\n",
       " 'assume': 885,\n",
       " 'sees': 886,\n",
       " 'floor': 887,\n",
       " 'mess': 888,\n",
       " 'shoes': 889,\n",
       " 'toys': 890,\n",
       " 'spread': 891,\n",
       " 'always': 892,\n",
       " 'against': 893,\n",
       " 'wall': 894,\n",
       " 'leaving': 895,\n",
       " 'hole': 896,\n",
       " 'walks': 897,\n",
       " 'tends': 898,\n",
       " 'trying': 899,\n",
       " 'comfort': 900,\n",
       " 'mall': 901,\n",
       " 'minute': 902,\n",
       " 'shopping': 903,\n",
       " 'paper': 904,\n",
       " 'lot': 905,\n",
       " 'full': 906,\n",
       " 'looking': 907,\n",
       " 'space': 908,\n",
       " 'notices': 909,\n",
       " 'finds': 910,\n",
       " 'spot': 911,\n",
       " 'park': 912,\n",
       " 'leaves': 913,\n",
       " 'calling': 914,\n",
       " 'owner': 915,\n",
       " 'selfish': 916,\n",
       " 'leave': 917,\n",
       " 'closing': 918,\n",
       " 'already': 919,\n",
       " 'vehicle': 920,\n",
       " 'longer': 921,\n",
       " 'odd': 922,\n",
       " 'lukas': 923,\n",
       " 'yelling': 924,\n",
       " 'ensues': 925,\n",
       " 'cop': 926,\n",
       " 'problem': 927,\n",
       " 'blown': 928,\n",
       " 'happens': 929,\n",
       " 'jumps': 930,\n",
       " 'starts': 931,\n",
       " 'drives': 932,\n",
       " 'front': 933,\n",
       " 'chase': 934,\n",
       " 'pile': 935,\n",
       " 'development': 936,\n",
       " 'manages': 937,\n",
       " 'open': 938,\n",
       " 'grab': 939,\n",
       " 'before': 940,\n",
       " 'thugs': 941,\n",
       " 'spends': 942,\n",
       " 'rest': 943,\n",
       " 'night': 944,\n",
       " 'weapons': 945,\n",
       " 'kill': 946,\n",
       " 'performs': 947,\n",
       " 'admit': 948,\n",
       " 'ending': 949,\n",
       " 'fun': 950,\n",
       " 'flick': 951,\n",
       " 'moves': 952,\n",
       " 'nicely': 953,\n",
       " 'unlikely': 954,\n",
       " 'further': 955,\n",
       " 'hanzo': 956,\n",
       " 'able': 957,\n",
       " 'expectation': 958,\n",
       " 'fine': 959,\n",
       " 'neatly': 960,\n",
       " 'terms': 961,\n",
       " 'quality': 962,\n",
       " 'general': 963,\n",
       " 'entertainment': 964,\n",
       " 'screenplay': 965,\n",
       " 'expected': 966,\n",
       " 'rings': 967,\n",
       " 'changes': 968,\n",
       " 'formula': 969,\n",
       " 'driven': 970,\n",
       " 'humour': 971,\n",
       " 'giving': 972,\n",
       " 'assistants': 973,\n",
       " 'typically': 974,\n",
       " 'straight': 975,\n",
       " 'knowledge': 976,\n",
       " 'turns': 977,\n",
       " 'serving': 978,\n",
       " 'guard': 979,\n",
       " 'stolen': 980,\n",
       " 'unfolds': 981,\n",
       " 'corruption': 982,\n",
       " 'sleazy': 983,\n",
       " 'results': 984,\n",
       " 'fighter': 985,\n",
       " 'sexual': 986,\n",
       " 'force': 987,\n",
       " 'equally': 988,\n",
       " 'knowing': 989,\n",
       " 'likable': 990,\n",
       " 'almost': 991,\n",
       " 'ordinary': 992,\n",
       " 'beautifully': 993,\n",
       " 'rounded': 994,\n",
       " 'filled': 995,\n",
       " 'social': 996,\n",
       " 'conscience': 997,\n",
       " 'touching': 998,\n",
       " 'personal': 999,\n",
       " 'snake': 1000,\n",
       " 'series': 1001,\n",
       " ...}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 50 dim GloVe vectors\n",
      "Found 400000 word vectors.\n",
      "words not found in embeddings: 16\n",
      "initial_embeddings.shape : (4992, 50)\n"
     ]
    }
   ],
   "source": [
    "# GloVe 임베딩 초기화 - glove.6B.50d.txt pretrained 벡터 사용\n",
    "glove = GloVe(50)\n",
    "initial_embeddings = glove.get_embedding(preprocessor.word_index)\n",
    "print(f'initial_embeddings.shape : {initial_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.12868069,  0.10964358, -0.10515184, ..., -0.13136516,\n",
       "        -0.02682301,  0.11120054],\n",
       "       [ 0.21705   ,  0.46515   , -0.46757001, ..., -0.043782  ,\n",
       "         0.41012999,  0.1796    ],\n",
       "       ...,\n",
       "       [-0.77372003,  0.13817   , -1.18710005, ...,  0.061871  ,\n",
       "         0.39048001, -1.12639999],\n",
       "       [-1.06140006, -0.94668001,  0.019802  , ...,  0.65306997,\n",
       "         0.28865001,  0.031796  ],\n",
       "       [ 0.9833132 ,  0.89334826,  0.7481463 , ...,  0.49181514,\n",
       "         0.91040518,  0.59357639]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사전학습 GloVe로부터 단어 별(IMDB 데이터 텍스트) 임베딩 값 가져온 결과\n",
    "initial_embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 전이학습 모델 가져오기 - HandsOnO3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size = 43197  and the index of vocabulary words passed has 43195 words\n"
     ]
    }
   ],
   "source": [
    "# 모델 하이퍼파라미터 로드\n",
    "# HandsOn-03_Movie_Review.ipynb에서 아마존 리뷰 모델 학습 후 checkpoint 폴더에 저장\n",
    "model_json_path = os.path.join(config.MODEL_DIR, 'amazonreviews/model_06.json')\n",
    "amazon_review_model = DocumentModel.load_model(model_json_path)\n",
    "\n",
    "# 모델 가중치 로드\n",
    "model_hdf5_path = os.path.join(config.MODEL_DIR, 'amazonreviews/model_06.hdf5')\n",
    "amazon_review_model.load_model_weights(model_hdf5_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned_embeddings size : 43197\n",
      "4895 words are updated out of 4990\n"
     ]
    }
   ],
   "source": [
    "# 모델 임베딩 레이어 추출\n",
    "learned_embeddings = amazon_review_model.get_classification_model().get_layer('imdb_embedding').get_weights()[0]\n",
    "print(f'learned_embeddings size : {len(learned_embeddings)}')\n",
    "\n",
    "# 기존 GloVe 모델을 학습된 임베딩 행렬로 업데이트한다\n",
    "# params: word_index_dict, other_embedding, other_word_index\n",
    "glove.update_embeddings(preprocessor.word_index, \n",
    "                        np.array(learned_embeddings), \n",
    "                        amazon_review_model.word_index)\n",
    "\n",
    "# 업데이트된 임베딩을 얻는다\n",
    "initial_embeddings = glove.get_embedding(preprocessor.word_index)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 참고: 사전학습된 모델의 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 300)]                0         []                            \n",
      "                                                                                                  \n",
      " imdb_embedding (Embedding)  (None, 300, 50)              2159850   ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 300, 50)              0         ['imdb_embedding[0][0]']      \n",
      "                                                                                                  \n",
      " lambda (Lambda)             (None, 30, 50)               0         ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)           (None, 30, 50)               0         ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " lambda_2 (Lambda)           (None, 30, 50)               0         ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " lambda_3 (Lambda)           (None, 30, 50)               0         ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " lambda_4 (Lambda)           (None, 30, 50)               0         ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " lambda_5 (Lambda)           (None, 30, 50)               0         ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " lambda_6 (Lambda)           (None, 30, 50)               0         ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " lambda_7 (Lambda)           (None, 30, 50)               0         ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " lambda_8 (Lambda)           (None, 30, 50)               0         ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " lambda_9 (Lambda)           (None, 30, 50)               0         ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " word_conv (Conv1D)          (None, 26, 30)               7530      ['lambda[0][0]',              \n",
      "                                                                     'lambda_1[0][0]',            \n",
      "                                                                     'lambda_2[0][0]',            \n",
      "                                                                     'lambda_3[0][0]',            \n",
      "                                                                     'lambda_4[0][0]',            \n",
      "                                                                     'lambda_5[0][0]',            \n",
      "                                                                     'lambda_6[0][0]',            \n",
      "                                                                     'lambda_7[0][0]',            \n",
      "                                                                     'lambda_8[0][0]',            \n",
      "                                                                     'lambda_9[0][0]']            \n",
      "                                                                                                  \n",
      " k_max_pooling (KMaxPooling  (None, 30, 3)                0         ['word_conv[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " k_max_pooling_1 (KMaxPooli  (None, 30, 3)                0         ['word_conv[1][0]']           \n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " k_max_pooling_2 (KMaxPooli  (None, 30, 3)                0         ['word_conv[2][0]']           \n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " k_max_pooling_3 (KMaxPooli  (None, 30, 3)                0         ['word_conv[3][0]']           \n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " k_max_pooling_4 (KMaxPooli  (None, 30, 3)                0         ['word_conv[4][0]']           \n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " k_max_pooling_5 (KMaxPooli  (None, 30, 3)                0         ['word_conv[5][0]']           \n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " k_max_pooling_6 (KMaxPooli  (None, 30, 3)                0         ['word_conv[6][0]']           \n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " k_max_pooling_7 (KMaxPooli  (None, 30, 3)                0         ['word_conv[7][0]']           \n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " k_max_pooling_8 (KMaxPooli  (None, 30, 3)                0         ['word_conv[8][0]']           \n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " k_max_pooling_9 (KMaxPooli  (None, 30, 3)                0         ['word_conv[9][0]']           \n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 90, 1)                0         ['k_max_pooling[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)         (None, 90, 1)                0         ['k_max_pooling_1[0][0]']     \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)         (None, 90, 1)                0         ['k_max_pooling_2[0][0]']     \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)         (None, 90, 1)                0         ['k_max_pooling_3[0][0]']     \n",
      "                                                                                                  \n",
      " reshape_4 (Reshape)         (None, 90, 1)                0         ['k_max_pooling_4[0][0]']     \n",
      "                                                                                                  \n",
      " reshape_5 (Reshape)         (None, 90, 1)                0         ['k_max_pooling_5[0][0]']     \n",
      "                                                                                                  \n",
      " reshape_6 (Reshape)         (None, 90, 1)                0         ['k_max_pooling_6[0][0]']     \n",
      "                                                                                                  \n",
      " reshape_7 (Reshape)         (None, 90, 1)                0         ['k_max_pooling_7[0][0]']     \n",
      "                                                                                                  \n",
      " reshape_8 (Reshape)         (None, 90, 1)                0         ['k_max_pooling_8[0][0]']     \n",
      "                                                                                                  \n",
      " reshape_9 (Reshape)         (None, 90, 1)                0         ['k_max_pooling_9[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 90, 10)               0         ['reshape[0][0]',             \n",
      "                                                                     'reshape_1[0][0]',           \n",
      "                                                                     'reshape_2[0][0]',           \n",
      "                                                                     'reshape_3[0][0]',           \n",
      "                                                                     'reshape_4[0][0]',           \n",
      "                                                                     'reshape_5[0][0]',           \n",
      "                                                                     'reshape_6[0][0]',           \n",
      "                                                                     'reshape_7[0][0]',           \n",
      "                                                                     'reshape_8[0][0]',           \n",
      "                                                                     'reshape_9[0][0]']           \n",
      "                                                                                                  \n",
      " sentence_embeddings (Permu  (None, 10, 90)               0         ['concatenate[0][0]']         \n",
      " te)                                                                                              \n",
      "                                                                                                  \n",
      " sentence_conv (Conv1D)      (None, 6, 16)                7216      ['sentence_embeddings[0][0]'] \n",
      "                                                                                                  \n",
      " k_max_pooling_10 (KMaxPool  (None, 16, 4)                0         ['sentence_conv[0][0]']       \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " document_embedding (Flatte  (None, 64)                   0         ['k_max_pooling_10[0][0]']    \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " gaussian_noise (GaussianNo  (None, 64)                   0         ['document_embedding[0][0]']  \n",
      " ise)                                                                                             \n",
      "                                                                                                  \n",
      " hidden_0 (Dense)            (None, 64)                   4160      ['gaussian_noise[0][0]']      \n",
      "                                                                                                  \n",
      " final (Dense)               (None, 1)                    65        ['hidden_0[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2178821 (8.31 MB)\n",
      "Trainable params: 2178821 (8.31 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "amazon_review_model.get_classification_model().summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 레이어 별 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_1',\n",
       " 'imdb_embedding',\n",
       " 'dropout',\n",
       " 'lambda',\n",
       " 'lambda_1',\n",
       " 'lambda_2',\n",
       " 'lambda_3',\n",
       " 'lambda_4',\n",
       " 'lambda_5',\n",
       " 'lambda_6',\n",
       " 'lambda_7',\n",
       " 'lambda_8',\n",
       " 'lambda_9',\n",
       " 'word_conv',\n",
       " 'k_max_pooling',\n",
       " 'k_max_pooling_1',\n",
       " 'k_max_pooling_2',\n",
       " 'k_max_pooling_3',\n",
       " 'k_max_pooling_4',\n",
       " 'k_max_pooling_5',\n",
       " 'k_max_pooling_6',\n",
       " 'k_max_pooling_7',\n",
       " 'k_max_pooling_8',\n",
       " 'k_max_pooling_9',\n",
       " 'reshape',\n",
       " 'reshape_1',\n",
       " 'reshape_2',\n",
       " 'reshape_3',\n",
       " 'reshape_4',\n",
       " 'reshape_5',\n",
       " 'reshape_6',\n",
       " 'reshape_7',\n",
       " 'reshape_8',\n",
       " 'reshape_9',\n",
       " 'concatenate',\n",
       " 'sentence_embeddings',\n",
       " 'sentence_conv',\n",
       " 'k_max_pooling_10',\n",
       " 'document_embedding',\n",
       " 'gaussian_noise',\n",
       " 'hidden_0',\n",
       " 'final']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i._name for i in amazon_review_model.get_classification_model().layers]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. IMDB 전이학습 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size = 4992  and the index of vocabulary words passed has 4990 words\n"
     ]
    }
   ],
   "source": [
    "# 분류 모델 생성 : IMDB 리뷰 데이터를 입력받아 이진분류를 수행하는 모델 생성\n",
    "imdb_model = DocumentModel(vocab_size=preprocessor.get_vocab_size(),\n",
    "                           word_index = preprocessor.word_index,\n",
    "                           num_sentences=Preprocess.NUM_SENTENCES,     \n",
    "                           embedding_weights=initial_embeddings,\n",
    "                           embedding_regularizer_l2 = 0.0,\n",
    "                           conv_activation = 'tanh',\n",
    "                           train_embedding = True,   # 임베딩 레이어의 가중치 학습함\n",
    "                           learn_word_conv = False,  # 단어 수준 conv 레이어의 가중치 학습 안 함\n",
    "                           learn_sent_conv = False,  # 문장 수준 conv 레이어의 가중치 학습 안 함\n",
    "                           hidden_dims=64,                                        \n",
    "                           input_dropout=0.1, \n",
    "                           hidden_layer_kernel_regularizer=0.01,\n",
    "                           final_layer_kernel_regularizer=0.01)\n",
    "\n",
    "# 가중치 업데이트 : 생성한 imdb_model 모델에서 다음의 각 레이어들의 가중치를 위에서 로드한 가중치로 갱신한다\n",
    "for l_name in ['word_conv','sentence_conv','hidden_0', 'final']:\n",
    "    new_weights = amazon_review_model.get_classification_model().get_layer(l_name).get_weights()\n",
    "    imdb_model.get_classification_model().get_layer(l_name).set_weights(weights=new_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 모델 학습 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.66842, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "10/10 - 1s - loss: 1.6150 - accuracy: 0.5861 - val_loss: 1.6684 - val_accuracy: 0.3077 - 1s/epoch - 118ms/step\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.66842 to 1.51567, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "10/10 - 0s - loss: 1.4795 - accuracy: 0.5772 - val_loss: 1.5157 - val_accuracy: 0.3077 - 247ms/epoch - 25ms/step\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.51567 to 1.43445, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "10/10 - 0s - loss: 1.3918 - accuracy: 0.5812 - val_loss: 1.4344 - val_accuracy: 0.5385 - 245ms/epoch - 24ms/step\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 1.43445 to 1.41991, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "10/10 - 0s - loss: 1.3125 - accuracy: 0.5643 - val_loss: 1.4199 - val_accuracy: 0.3846 - 260ms/epoch - 26ms/step\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 1.41991 to 1.33160, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "10/10 - 0s - loss: 1.2472 - accuracy: 0.5909 - val_loss: 1.3316 - val_accuracy: 0.3077 - 262ms/epoch - 26ms/step\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 1.33160 to 1.29793, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "10/10 - 0s - loss: 1.1807 - accuracy: 0.5877 - val_loss: 1.2979 - val_accuracy: 0.3846 - 260ms/epoch - 26ms/step\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 1.29793 to 1.25191, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "10/10 - 0s - loss: 1.1279 - accuracy: 0.5861 - val_loss: 1.2519 - val_accuracy: 0.3846 - 262ms/epoch - 26ms/step\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 1.25191 to 1.21573, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "10/10 - 0s - loss: 1.0731 - accuracy: 0.5958 - val_loss: 1.2157 - val_accuracy: 0.3846 - 272ms/epoch - 27ms/step\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 1.21573 to 1.17166, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "10/10 - 0s - loss: 1.0258 - accuracy: 0.5918 - val_loss: 1.1717 - val_accuracy: 0.3846 - 275ms/epoch - 28ms/step\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 1.17166 to 1.15432, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "10/10 - 0s - loss: 0.9784 - accuracy: 0.5990 - val_loss: 1.1543 - val_accuracy: 0.3077 - 283ms/epoch - 28ms/step\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 1.15432 to 1.08499, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "10/10 - 0s - loss: 0.9537 - accuracy: 0.6031 - val_loss: 1.0850 - val_accuracy: 0.3846 - 300ms/epoch - 30ms/step\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 1.08499 to 1.07380, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "10/10 - 0s - loss: 0.9087 - accuracy: 0.6103 - val_loss: 1.0738 - val_accuracy: 0.3846 - 309ms/epoch - 31ms/step\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 1.07380 to 1.05860, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "10/10 - 0s - loss: 0.8804 - accuracy: 0.5966 - val_loss: 1.0586 - val_accuracy: 0.3846 - 307ms/epoch - 31ms/step\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss did not improve from 1.05860\n",
      "10/10 - 0s - loss: 0.8494 - accuracy: 0.6055 - val_loss: 1.0601 - val_accuracy: 0.3077 - 306ms/epoch - 31ms/step\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 1.05860 to 1.05512, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "10/10 - 0s - loss: 0.8230 - accuracy: 0.6063 - val_loss: 1.0551 - val_accuracy: 0.3077 - 314ms/epoch - 31ms/step\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 1.05512 to 1.05018, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "10/10 - 0s - loss: 0.8018 - accuracy: 0.6160 - val_loss: 1.0502 - val_accuracy: 0.3077 - 328ms/epoch - 33ms/step\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 1.05018 to 1.03959, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "10/10 - 0s - loss: 0.7838 - accuracy: 0.6055 - val_loss: 1.0396 - val_accuracy: 0.3077 - 332ms/epoch - 33ms/step\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss did not improve from 1.03959\n",
      "10/10 - 0s - loss: 0.7619 - accuracy: 0.6128 - val_loss: 1.0426 - val_accuracy: 0.3077 - 319ms/epoch - 32ms/step\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 1.03959 to 1.00622, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "10/10 - 0s - loss: 0.7493 - accuracy: 0.6192 - val_loss: 1.0062 - val_accuracy: 0.3077 - 330ms/epoch - 33ms/step\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss did not improve from 1.00622\n",
      "10/10 - 0s - loss: 0.7335 - accuracy: 0.6112 - val_loss: 1.0202 - val_accuracy: 0.3077 - 327ms/epoch - 33ms/step\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 1.00622 to 0.99815, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "10/10 - 0s - loss: 0.7237 - accuracy: 0.6031 - val_loss: 0.9981 - val_accuracy: 0.3077 - 317ms/epoch - 32ms/step\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.99815\n",
      "10/10 - 0s - loss: 0.7059 - accuracy: 0.6168 - val_loss: 1.0285 - val_accuracy: 0.3077 - 320ms/epoch - 32ms/step\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.99815\n",
      "10/10 - 0s - loss: 0.6991 - accuracy: 0.6184 - val_loss: 1.0208 - val_accuracy: 0.3077 - 328ms/epoch - 33ms/step\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.99815\n",
      "10/10 - 0s - loss: 0.6927 - accuracy: 0.6128 - val_loss: 1.0454 - val_accuracy: 0.3077 - 323ms/epoch - 32ms/step\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.99815\n",
      "10/10 - 0s - loss: 0.6808 - accuracy: 0.6184 - val_loss: 1.0289 - val_accuracy: 0.3077 - 333ms/epoch - 33ms/step\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.99815\n",
      "10/10 - 0s - loss: 0.6737 - accuracy: 0.6249 - val_loss: 1.0266 - val_accuracy: 0.3077 - 316ms/epoch - 32ms/step\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.99815\n",
      "10/10 - 0s - loss: 0.6670 - accuracy: 0.6217 - val_loss: 1.0358 - val_accuracy: 0.3077 - 331ms/epoch - 33ms/step\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.99815\n",
      "10/10 - 0s - loss: 0.6615 - accuracy: 0.6257 - val_loss: 1.0267 - val_accuracy: 0.3077 - 341ms/epoch - 34ms/step\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.99815\n",
      "10/10 - 0s - loss: 0.6627 - accuracy: 0.6281 - val_loss: 1.0163 - val_accuracy: 0.3077 - 328ms/epoch - 33ms/step\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.99815\n",
      "10/10 - 0s - loss: 0.6492 - accuracy: 0.6306 - val_loss: 1.0589 - val_accuracy: 0.3077 - 332ms/epoch - 33ms/step\n"
     ]
    }
   ],
   "source": [
    "# 모델 컴파일              \n",
    "imdb_model.get_classification_model().compile(loss=\"binary_crossentropy\", \n",
    "                                              optimizer='rmsprop',\n",
    "                                              metrics=[\"accuracy\"])\n",
    "\n",
    "# callback (1) - 체크포인트\n",
    "checkpointer = ModelCheckpoint(filepath=train_params.model_file_path,\n",
    "                                verbose=1,\n",
    "                                save_best_only=True,\n",
    "                                save_weights_only=True)\n",
    "\n",
    "# callback (2) - 조기종료\n",
    "early_stop = EarlyStopping(patience=2)\n",
    "\n",
    "# 학습 시작\n",
    "imdb_model.get_classification_model().fit(x_train, \n",
    "                                          y_train, \n",
    "                                          batch_size=train_params.batch_size,\n",
    "                                          epochs=train_params.num_epochs,\n",
    "                                          verbose=2,\n",
    "                                          validation_split=0.01,\n",
    "                                          callbacks=[checkpointer])\n",
    "\n",
    "# 모델 저장\n",
    "imdb_model._save_model(train_params.model_hyper_parameters)\n",
    "train_params.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 - 1s - loss: 0.7217 - accuracy: 0.5697 - 1s/epoch - 57ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7216610908508301, 0.5697199702262878]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 평가\n",
    "imdb_model.get_classification_model().evaluate(x_test, \n",
    "                                               y_test, \n",
    "                                               batch_size=train_params.batch_size*10,\n",
    "                                               verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 4s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_test = imdb_model.get_classification_model().predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.reset_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. 부정적인 감정 리뷰 & 예측결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> sentiment: 0\n",
      "Robin Williams is excellent in this movie and it is a pity the material is not enough of a match for him. This may work if you buy into the \"U-S-A! Number One!\" mentality but story wise nothing much happens. Quite a shame really since the movie is really trying to say something, and says it sincerely. It just doesn't pack enough emotional punch.\n"
     ]
    }
   ],
   "source": [
    "# i = (test_df[test_df['sentiment']==1]).index[25]\n",
    "i = 10\n",
    "print(f\"> sentiment: {test_df.loc[i,'sentiment']}\")\n",
    "print(test_df.loc[i,'review'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5160389], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측결과\n",
    "pred_test[i]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. 긍정적인 감정 리뷰 & 예측결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> sentiment: 0\n",
      "Catherine Zeta-Jones and Aaron Eckhart star in a \"romantic\" drama about an uptight chef played by Zeta-Jones, who ends up carrying for her niece when her sister is killed in a car crash. While she's out taking care of family matters she's replaced by Eckhart.Unfunny maudlin tale with no chemistry between the leads (she's a dead fish and he's okay, but not much of anything). Watching this I was wondering why anyone would want to see this since Zeta-Jones' character is so unlikable. Come on she's so obsessed with cooking and being the best all she does is cook for her therapist or talk about food. Ugh. I won't use any of the numerous puns that come to mind. I couldn't finish it.\n"
     ]
    }
   ],
   "source": [
    "i = (test_df[test_df['sentiment']==0]).index[20]\n",
    "# i = np.argmax(pred_test)\n",
    "i  = 27\n",
    "print(f\"> sentiment: {test_df.loc[i,'sentiment']}\")\n",
    "print(test_df.loc[i,'review'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5160389], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측결과\n",
    "pred_test[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>23881</td>\n",
       "      <td>Catherine Zeta-Jones and Aaron Eckhart star in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>24360</td>\n",
       "      <td>This son of a son of a sequel was terrible to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                                             review  sentiment\n",
       "27  23881  Catherine Zeta-Jones and Aaron Eckhart star in...          0\n",
       "28  24360  This son of a son of a sequel was terrible to ...          0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.loc[[27,28],:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f541650a259d3c85c16fa389922248120f711c850d1b246e80025daa1aee6568"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
