# 텍스트 분석에서의 전이학습

# I. 들어가기 전에. 텍스트 임베딩
- 언어를 연산 가능하도록 숫자 벡터로 표현하자
- 임베딩의 장점
    - 의미적 유사성 캡처: 임베딩은 단어나 문장 간의 의미적 유사성을 보존하도록 설계 -> 벡터 공간에서 유사한 텍스트는 비슷한 임베딩을 가짐
    - 기계 학습 모델 적용: 임베딩된 텍스트는 수치형 데이터로 표현되므로 다양한 기계 학습 모델에 입력으로 사용 가능 -> 텍스트 분류, 감정 분석, 기계 번역, 질문 응답 등에 적용
    - 차원 축소 및 특징 학습: 임베딩은 원래 텍스트의 차원을 상당히 높은 차원에서 저차원의 밀집 벡터로 축소시켜주기 때문에 특징 추출 및 학습에 유용
- 작업의 특성에 따라 적합한 임베딩을 선택하는 것이 중요

## [ 다양한 기법들 ]
- 텍스트 임베딩은 텍스트 단위(단어, 문장, 문서)에 따라 임베딩 기법이 다르다. 
- 단어의 맥락 파악 관점에서 보면, 아래 언급된 방법론들 중 [3. 문서 임베딩]의 Bow와 TF-IDF를 제외하면 모두 단어의 맥락을 파악할 수 있다.
## 1. Word Embeddings (단어 임베딩)
- **Word2Vec:** 
    - 개념: Word2Vec은 단어를 벡터로 표현하는 기법으로, CBOW(Continuous Bag of Words)와 Skip-gram이 있음. CBOW는 주변 단어들을 사용하여 중심 단어를 예측하고, Skip-gram은 중심 단어를 사용하여 주변 단어를 예측.
        - 예제. "I like to learn machine"<br/>
        . CBOW: 주어진 문맥 "I like to learn"에서 "machine"을 예측하는 것이 목표 <br/>
        . skip-gram: 주어진 중심 단어 "machine"에서 주변 단어 ["I", "like", "to", "learn"]을 예측하는 것이 목표
        
    - 장점: 대규모 텍스트 데이터에서 효과적으로 학습되며, 단어 간 의미적 유사성을 잘 캡처.
    - 단점: 희소한 단어에 대한 표현이 부족할 수 있으며, 문장의 전체적인 문맥을 고려하지 않음
- **GloVe (Global Vectors for Word Representation):**
    - **개념:** 단어 간의 동시 등장 통계를 이용하여 단어 간의 관계를 학습하는 방법. 전체 말뭉치의 통계 정보를 활용하여 각 단어의 벡터를 생성.
    - **장점:** 단어 간 의미 관계를 잘 파악하며, 단어의 빈도에 대한 정보를 잘 반영.
    - **단점:** Word2Vec과 유사하게 문맥을 고려하지 않음.
- **FastText Embeddings:**
    - **개념:** FastText는 Facebook에서 개발한 단어 임베딩 기법으로, 단어를 subword 단위로 나누어 각각의 subword에 대한 벡터를 학습. <br/>
    -> 희소한 단어에 대한 강건한 표현을 제공하며, 오탈자에 대응 가능.
    - **장점:** 강건한 희소 단어 표현, 오탈자 강건성, 언어 별 일반화 가능성, 빠른 학습 속도와 같은 특성을 갖추어, 특히 다양한 언어 및 텍스트 환경에서 효과적으로 작동.
    - **단점:**: 의미적 한계와 문맥 파악의 한계, 추상적 표현 부족, 대용량 데이터셋 학습 시 메모리 사용량 증가와 같은 한계점이 존재. 특히 문장 전체의 복잡한 문맥을 다루는 데에 한계가 있음.

## 2. Sentence Embeddings (문장 임베딩)
- **Doc2Vec (Paragraph Vector):** 
    - **개념:** 단어 및 문장을 벡터로 임베딩하는 방법으로, 각 문장을 대표하는 벡터를 생성.
    - **장점:** 문장 간 의미 관계를 캡처하며, 문서 수준의 임베딩을 제공.
    - **단점:** 대규모 데이터에서 학습에 시간이 오래 걸릴 수 있음.
- **InferSent:** 
    - **개념:** Skip-thought 모델을 기반으로 각 문장을 벡터로 임베딩. 문장 간의 의미적 유사성을 학습.
    - **장점:** 문장 간의 의미 관계를 잘 파악하며, 감정 분석 등의 작업에 유용.
    - **단점:** 상대적으로 많은 데이터가 필요하며, 학습에 시간이 소요될 수 있음.

## 3. Document Embeddings (문서 임베딩)
- **Bag-of-Words (BoW):**
   - **개념:** 문서를 단어의 집합으로 표현하고, 각 단어의 등장 여부에 따라 이진벡터 또는 단어 빈도를 사용하는 방법.
   - **장점:** 간단하고 직관적이며, 단어의 순서에 대한 고려가 없어도 됨.
   - **단점:** 문맥 정보가 손실되며, 단어의 의미적 유사성을 캡처하지 못함.
        - 부분적으로 단어 순서 고려: **n-gram**(연속적인 n개의 단어 나열) <br/>
        ex. "The cat in the hat" -> ["The cat", "cat in", "in the", "the hat"]. <br/>
        . 장점 - 문맥을 일부 보존: N-gram을 사용하면 단어들 간의 일부 문맥을 고려할 수 있습니다. 특히 2-gram 이상을 사용하면 인접한 단어들 간의 관계를 파악할 수 있습니다. <br/>
        . 단점 - 차원의 폭증: 특히 큰 n에 대해서는 많은 단어 조합이 발생하므로, BoW 행렬의 차원이 급격히 증가합니다. 이는 희소성과 메모리 요구량 증가로 이어질 수 있습니다. <br/>

- **TF-IDF (Term Frequency-Inverse Document Frequency):**
   - **개념:**  Term Frequency (단어 빈도)와 IDF를 곱한 값, 각 단어에 가중치를 부여하여 문서를 벡터로 표현하는 방법으로, 단어의 중요성을 고려.
        - IDF는 로그를 사용하여 단어의 문서 빈도에 대한 역수를 계산 <br/> 특정 단어가 많은 문서에 나타나면 IDF는 작아지고, 특정 단어가 적은 문서에 나타나면 IDF는 커짐. <br/> -> 단어들에 대해서는 낮은 가중치를 부여하고, 특이하거나 중요한 단어에 대해서는 높은 가중치를 부여하는 효과
   - **장점:** 중요한 단어에 높은 가중치를 부여하여 의미 있는 특성을 추출.
   - **단점:** 문맥과 단어의 순서를 고려하지 않음.
- 참고. BoW나 TF-IDF는 주로 문서 간 단어 빈도의 유사성을 측정하는 데 사용되며, 단어의 의미적 유사성을 표현하기에는 부적절

## 4. 단어, 문장, 문서 단위 포괄 임베딩
- **Transformer-based Embeddings:**
   - **BERT (Bidirectional Encoder Representations from Transformers):** 양방향 Transformer 아키텍처를 사용하여 사전 학습된 문맥을 고려한 단어 임베딩을 생성.
   - **GPT (Generative Pre-trained Transformer):** 트랜스포머 모델을 사용하여 텍스트를 생성하고 이를 통해 문맥을 이해하는 임베딩을 만듦.

# II. 텍스트 데이터의 전이학습에서 주의할 점




# 참고. 활용할만한 데이터셋
- IMDB: 영화 리뷰(train set 25,000 & test set 25,000), binary 감성 분류
- 로이터 데이터: 문서 classification(90 labels), train set 9,584 & test set 3,744, unbalanced class
- 20 뉴스 그룹 데이터: news groups, 6 main categories, 20 sub categories, sklearn.datasets 라이브러리에 있음


